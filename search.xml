<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>TX2刷机Jetpack4.2教程</title>
      <link href="2021/05/01/tx2-shua-ji-jetpack4-2-jiao-cheng/"/>
      <url>2021/05/01/tx2-shua-ji-jetpack4-2-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p><strong>TX2刷机Jetpack4.2刷机教程</strong><br>文章参考以下链接：<br><a href="https://blog.csdn.net/zt1091574181/article/details/88847775">https://blog.csdn.net/zt1091574181/article/details/88847775</a><br><a href="https://blog.csdn.net/YiYeZhiNian/article/details/94407065">https://blog.csdn.net/YiYeZhiNian/article/details/94407065</a><br><a href="https://blog.csdn.net/qq_41587270/article/details/97623350">https://blog.csdn.net/qq_41587270/article/details/97623350</a><br>这几篇都还可以。<br>这几篇是我结合起来后自己再去写一篇关于自己所安装的过程，如果其中有侵权的地方请联系我，谢谢！</p><p>TX2刷机是一般入手TX2后比较常做的事情，因为一般刚买来的TX2因为内置的Ubuntu版本和一些安装的包都比较旧所以一般我们会选择去通过刷机刷到较新的版本然后获得更新的包和库。</p><p>首先刷机你需要一台安装有Ubuntu系统或装有Ubuntu虚拟机的电脑，这里不讲如何装Ubuntu双系统或虚拟机，网上教程很多每个人电脑都不一样。（这里我自己的电脑装的是Ubuntu16.04+Win10双系统没有装虚拟机所以这里我不讲虚拟机的刷机教程，还有Ubuntu的版本不是问题不管是16.04还是18.04都一样，个人认为电脑只是个刷机工具媒介而已，在Ubuntu上下载好SDK后通过USB转micro-usb传到TX2上。）接下来我们开始刷机过程。</p><p>一.下载NVIDIA SDKManager<br>     从官网上下载<a href="https://developer.nvidia.com/embedded/jetpack">（官网地址）</a><br>     <img src="https://img-blog.csdnimg.cn/20191111215830717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>进入后点击SDK Manager下载，然后会跳到一个注册界面，按要求注册一个账号就行，账号要记得因为后面要用到。然后会得到一个.deb的文件包，然后进入/home/下载里面找到那个包直接双击打开就行，可能需要等一些些时间，然后出现下面画面点击安装。<br><img src="https://img-blog.csdnimg.cn/20191111220430461.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>安装好之后可以看到下面的界面，输入刚刚创建的NVIDIA账号密码<br><img src="https://img-blog.csdnimg.cn/20191111220625173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>然后进入STEP1，选择下Target Hardware中的TX2记得选啊不要选到别的去了。然后system要选Linux的Jetpack4.2<br><img src="https://img-blog.csdnimg.cn/20191111220922365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>点击Continue进入STEP2，什么都不用勾选直接点击继续。（其实这里多说一句话，如果你刷的是Jetpack4.2.2的话它是比4.2多出了个Deepstream和Tensorflow的包，但是我其实第一次就是装4.2.2的但后面就是安装过程一半老是出现error或者是卡进度条一直不动。于是乎我就索性都退出去安装4.2版本的然后比较顺利。）<br>接下来就是STEP3安装过程<br><img src="https://img-blog.csdnimg.cn/2019111122135130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"></p><p>这个过程极其漫长然后很艰辛中间可能会出现安装失败或者是卡进度条就是一直进度不动的那种，而且关于网络的连接也是个问题，我尝试用过WI-FI，网线，手机热点，但我看网上说手机热点最好其实我觉得都差不多啦不过热点确实好那么一丢丢比较稳定点，还有就是网上也有人说换个源可以下的快一点但是我也试过了，其实我觉得没差还不如不换，当时换源的时候还出了事故。（不管是下载进度还是安装进度中出现问题，不要放弃就退出去重新再登陆一下再点击一遍过去，下载过的会保存不会重复下载）<br>如果有人要尝试换源我也给出换源的方法，换的是阿里源。<br>1.备份</p><pre class="line-numbers language-none"><code class="language-none">sudo cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.backu<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2.修改添加阿里源</p><pre class="line-numbers language-none"><code class="language-none">sudo cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.backu<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3.出现一个文件然后在最底下添加以下代码</p><pre class="line-numbers language-none"><code class="language-none"># 阿里云deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiversedeb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiversedeb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiverse<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当你成功下载完以后恭喜你！<br>下载好之后，用USB转micro-usb线将电脑和TX2相连接，其中USB接电脑Micro-usb接TX2，这边记住要用买TX2时官方送的线，我当时试了好几根线SDK就一直报错说烧不进去，这里我展示下官方的线。（两个头上有绿色标志的）<br><img src="https://img-blog.csdnimg.cn/2019111122295794.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>接下来就是要把TX2进入Recovery模式，操作方法如下：<br>1.先把电源要拔掉然后再直接插上<br>2.按下S4 power键一下后松开开启<br>3.紧接着马上一直按住S3键Force Recovery不放<br>4.还是按住S3不放的同时赶紧按一下S1键Reset后松开<br>5.然后心中默数两秒后松开S3键<br>然后就进入了Recovery模式此时接TX2的显示屏上是一团黑的，正常。<br>这时候在进入Recovery和USB线接上的情况下，在电脑端的终端输入lsusb看看有没有NVIDIA crop，好像是会出现在中间一行，反正就出现的几行找一下有没有这个，有的话就是电脑和TX2连接上了。</p><p><img src="https://img-blog.csdnimg.cn/201911112244444.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>如果出现这个记得不要选择自动，要选择另外一个手动</p><p>在刚刚的STEP3下载好后就是会提示你要不要flash你的TX2你就顺着提示来点击就行然后flash进去，这里也要等的挺长一段时间的。在flash过程其实也会出现erro或卡进度条情况，一样的重来，反正系统有存档不会重复下载的。</p><p>我记得flash后TX2的显示屏会出现界面然后提示你一步步开始创建新的用户名啥的，反正很简单跟着提示走，对了在创建用户名和密码时候要记住因为在电脑端会让你输入TX2的用户名和密码然后进行核对匹配。<br>最后应该就没有了，最后安装好之后就会出现这样<br><img src="https://img-blog.csdnimg.cn/20191111224947799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>然后这就是整个刷机过程了，可能我写的不是很好但是希望其他人能够少踩点坑。不过踩坑也挺好的加深印象。<br>顺带附上刷机后新的TX2如何小风扇散热。<br>Jetpack4.2以上的打开散热扇需要</p><pre class="line-numbers language-none"><code class="language-none">cd &#x2F;usr&#x2F;bin&#x2F;sudo .&#x2F;jetson_clocks<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> NVIDIA </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NVIDIA </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于Python的HOG+SVM行人识别</title>
      <link href="2021/05/01/ji-yu-python-de-hog-svm-xing-ren-shi-bie/"/>
      <url>2021/05/01/ji-yu-python-de-hog-svm-xing-ren-shi-bie/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先我将简单阐述一下HOG和SVM的原理，当然重点主要是HOG对于SVM已经有很多的资料讲述的很清楚我觉得此处没有必要再详细讲解。</p><ol><li>   HOG特征提取原理</li><li>   SVM简单原理概述</li><li>   基于Python的HOG+SVM的行人识别</li></ol><p><strong>一、HOG特征提取原理</strong><br>首先先讲一下HOG是什么和HOG特征提取的步骤吧。<br>首先HOG就是梯度方向直方图  (Histogram of Oriented Gradient, HOG) ，HOG 特征是直接将图像像素点的方向梯度作为图像特征，包括梯度大小和方向。说实话在我自己看来的话，其实HOG有点像边缘提取把一个物体的轮廓可以用梯度的方法很好的描绘出来，而轮廓说实在的就是特征了吧。</p><p>然后就是HOG特征提取的步骤。其实网上已经有很多HOG特征提取原理的概述但是我看了这么多好像都是大同小异，在我刚开始看的时候真的看了很多然后发现讲来讲去都是那些东西对于刚理解刚入手的人来说是有点困难但这也不可避免，看不懂真的就要多看多看多坎=看几遍！！多看+思考就会发现原来HOG不会太难。这是HOG特征提取的一个经典步骤图：<br><img src="https://img-blog.csdnimg.cn/20200204112032432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>1.图像标准化，在其他地方可能也会成为对图像进行伽马处理。就是对每个图像中的像素点进行n次方，其中的n称为伽马系数，一般n=0.5。伽马值，是对图像的优化调整，是亮度和对比度的辅助功能。其实好像如果做静态图片处理的时候没怎么用到这一步欸。</p><p>2.计算梯度，梯度的计算是要求每个像素点水平和垂直方向的梯度值<br><img src="https://img-blog.csdnimg.cn/20200204112837753.png"><br>I（x，y）是指在（x，y）处的像素点的值，然后就得到了x和y方向上的梯度值<br>然后就可以求梯度方向和幅值了<br><img src="https://img-blog.csdnimg.cn/20200204113032186.png"><br>3.接下来要说的是重点就是Win，block，blockStride，cell，cellBin<br><img src="https://img-blog.csdnimg.cn/20200204113305521.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>win就是一个检测窗口一般的化我们都是习惯上采用64<em>128因为这是用来检测行人或车辆上研究出来比较适合的一个参数值1：2或2：1。<br>block是在win窗口里面移动的一个16</em>16的块<br>blockStride就是块每次移动的步长了一般为8<em>8<br>cell就是block里面一个8</em>8的元组，一个块里面含有四个cell<br>一个cell里面一般分为9个bin也就是9个方向，下面这个图是方向分类的一个圆图<br><img src="https://img-blog.csdnimg.cn/20200204114219992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>可能有人要问为什么一定是这些参数，其实我当时也有质疑过，但后来发现这些参数的设定是有一定道理的，这是人家在经过多次的试验之后得出来得一个比较合理通用的一个参数。</p><p>4.然后就像以上那样求出所有像素点得特征后我们来计算一下。一张图像一般裁剪成64<em>128（行人检测）然后一共有（（64-16）/8+1）</em>（（128-16）/8+1）=105个块，一个块有4个cell，一个cell有9个bin，那么一共有105x4x9=3780个bin，也就是一张图像会得到3780个数字也就是这张图象特征维度。然后我们会把这个3780个数放在一个数组里面。</p><p><strong>二、SVM简单概述</strong><br>SVM就是支持向量机，它在做线性二分类问题上有着很突出的性能优势，当然在其他方面也是一样，这也要配合于核函数，可以说核函数在机器学习里面十分的重要和强大。<br>SVM就是找到一个分界面把两类数据能够分离开来，然后使得两边最靠近的点的距离最大。<br>函数间隔y(wx+b)：|wx+b|表示点x距离超平面的远近，wx+b的符号与y的符号是否一致表示分类是否正确，所以y(wx+b)就表示分类的正确性和确信度。<br>max(几何间隔) -&gt; max(函数间隔/||w||) -&gt; min(1/||w||2)，即求解凸二次优化问题（最大间隔法求解）或应用拉格朗日对偶性 -&gt; 求解对偶问题(SMO算法)。</p><p>核函数的作用就是处理线性分类问题。例如需要用超曲面来分开所有实例点。原理是使用一个变换将原空间的数据映射到新空间。在处理非线性分类问题的时候，实例点无法用线性模型正确分开，需要使用和核函数将数据映射到一个新的空间，在新空间里用线性分类学习方法求得超平面。</p><p>以上就是SVM的简介如果需要更详细的叙述可以查看书籍或网上资料有很多关于SVM的细节上的推导。</p><p><strong>三、基于Python的HOG+SVM行人检测</strong><br>HOG+SVM的基本步骤其实都大同小异这里我整理了下自己的。<br>1.提取获得正样本和负样本数据当然还有测试集数据，正样本就是行人的图像标签为+1而负样本就是非行人的图像标签为-1。<br>2.开始获得正样本和负样本的HOG数据并且保存在数组里面，还要再设一个标签数组用来存放正负样本的标签值<br>3.svm对象的训练，并且保存下训练后的数据文件<br>4.加载测试对象，载入数据文件，计算HOG，用svm判断。</p><p>下面上代码</p><pre class="line-numbers language-none"><code class="language-none">import cv2import numpy as npdef get_svm_detector(svm):    sv &#x3D; svm.getSupportVectors()    #SVM支持向量    rho, _, _ &#x3D; svm.getDecisionFunction(0)#SVM的决策函数    sv &#x3D; np.transpose(sv)    return np.append(sv, [[-rho]], 0)  #得到支持向量和决策函数系数的一个数组#SVM的分类平面计算PosNum&#x3D;3548NegNum&#x3D;10000winSize&#x3D;(64,128)blockSize&#x3D;(16,16)blockStride&#x3D;(8,8)cellSize&#x3D;(8,8)nBin&#x3D;9#初始的值设定hog&#x3D;cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nBin)#HOG对象的创建初始化svm&#x3D;cv2.ml.SVM_create()#SVM对象的创建featureNum &#x3D; int(((128 - 16) &#x2F; 8 + 1) * ((64 - 16) &#x2F; 8 + 1) * 4 * 9)#计算HOG的维度总数即特征数featureArray &#x3D; np.zeros(((PosNum + NegNum), featureNum), np.float32)labelArray &#x3D; np.zeros(((PosNum + NegNum), 1), np.int32)#构建两个数组分别是放特征的和放标签的for i in range(0,PosNum):    n&#x3D;str(i+1)    fileName&#x3D;&quot;C:&#x2F;&#x2F;Users&#x2F;&#x2F;dell&#x2F;&#x2F;Desktop&#x2F;&#x2F;DATA&#x2F;&#x2F;pos&#x2F;&#x2F;person&quot;+n.zfill(6)+&quot;.jpg&quot;    img&#x3D;cv2.imread(fileName)    hist&#x3D;hog.compute(img,(8,8))    for j in range(0,featureNum):        featureArray[i,j]&#x3D;hist[j]    labelArray[i,0]&#x3D;1#正样本的HOG特征的计算并存进featureArray数组中for i in range(0,NegNum):    n&#x3D;str(i+1)    filename&#x3D;&quot;C:&#x2F;&#x2F;Users&#x2F;&#x2F;dell&#x2F;&#x2F;Desktop&#x2F;&#x2F;DATA&#x2F;&#x2F;neg&#x2F;&#x2F;noperson&quot;+n.zfill(6)+&quot;.jpg&quot;    img&#x3D;cv2.imread(filename)    hist&#x3D;hog.compute(img,(8,8))    for j in range(0,featureNum):        featureArray[i+PosNum,j]&#x3D;hist[j]    labelArray[i+PosNum,0]&#x3D;-1#负样本的HOG特征计算#SVM的参数初始化，这里用SVM线性分类器来做速度比较快svm.setCoef0(0)svm.setCoef0(0.0)svm.setDegree(3)criteria &#x3D; (cv2.TERM_CRITERIA_MAX_ITER + cv2.TERM_CRITERIA_EPS, 1000, 1e-3)svm.setTermCriteria(criteria)svm.setGamma(0)svm.setNu(0.5)svm.setP(0.1)svm.setC(0.01)svm.setType(cv2.ml.SVM_EPS_SVR)svm.setKernel(cv2.ml.SVM_LINEAR)svm.setC(0.01)#SVM对象参数的设置，核算子的设定等svm.train(featureArray, cv2.ml.ROW_SAMPLE, labelArray)#把计算得到的HOG数据放到SVM对象分类器里面进行训练HOG&#x3D;cv2.HOGDescriptor()HOG.setSVMDetector(get_svm_detector(svm))HOG.save(&#39;myHogDector.bin&#39;)#HOG和SVM结合的一个分类器并且把文件存下来savemyHog &#x3D; cv2.HOGDescriptor()myHog.load(&#39;myHogDector.bin&#39;)#开始加载上述的那个分类器imageSrc &#x3D; cv2.imread(&#39;C:&#x2F;&#x2F;Users&#x2F;&#x2F;dell&#x2F;&#x2F;Desktop&#x2F;&#x2F;2.jpg&#39;)#输入检测的图像rects,wei&#x3D;myHog.detectMultiScale(imageSrc,0,(4, 4),(32, 32), 1.05, 2)for (x, y, w, h) in rects:    cv2.rectangle(imageSrc, (x, y), (x + w, y + h), (0, 0, 255), 2)#对检测出来的分类为1的矩形进行遍历框出来cv2.imshow(&#39;dst&#39;, imageSrc)cv2.waitKey(0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面是效果图分别是非行人图像和行人图像，对于非行人图是不会有矩形框出的，而对于行人图像将会有矩形框出行人大致轮廓。<br><img src="https://img-blog.csdnimg.cn/20200204133811249.png"><br><img src="https://img-blog.csdnimg.cn/20200204133830880.png"><br><strong>四、结语与总结</strong><br>这一份代码只是比较草稿比较简单的一份，只是写明了HOG+SVM来用于检测的一个基本的步骤，其实还是有错误率存在这是由于两个原因，一个是可能自身的样本还不够大，第二个是这份是比较简单和草稿的代码里面的SVM训练只有一次，其实按道理应该要对负样本进行一次训练然后抽出那些分类错的记录下来纠正一下，但这些还没写好，这里可能代码效果还不是很好，请谅解。<br>可能本文写的不是那么好还是请各位谅解，本人只是想记录一下学习的基本过程步骤，本文如有雷同敬请谅解，毕竟自己也是看了很多的文章和书籍然后脑海里综合了很多。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YALE和COIL20数据集的可视化（Python实现）</title>
      <link href="2021/05/01/yale-he-coil20-shu-ju-ji-de-ke-shi-hua-python-shi-xian/"/>
      <url>2021/05/01/yale-he-coil20-shu-ju-ji-de-ke-shi-hua-python-shi-xian/</url>
      
        <content type="html"><![CDATA[<p>本文使用的数据集是YALE_32X32.mat和COIL20.mat数据集，这两个数据集应该是很容易就可以得到的。这里给出两个数据集链接<br><a href="https://pan.baidu.com/s/1611lz6Wt2eCR_yHrC_5yjA">YALE_32X32</a><br>提取码：94l2<br><a href="https://pan.baidu.com/s/1iZRZDB42rhIAMWsACoyAWw">COIL20</a><br>提取码：8cjn</p><p>用Python来可视化数据集其实很简单当然也可以用C++或Matlab可视化也可以的，但是本人偏向于喜欢用Python来而且Python的优点很多接口也很多很适合用来做人工智能。可视化数据集没有什么困难的，当然如果是新手来说，可能你首先一定要很清楚什么是图像，图像处理的一些基本概念和内容然后再去操作会更好。其实很早就想把自己半年来学习的东西总结了只不过一直没有时间刚好寒假总算有时间把当时刚开始接触人工智能的一些基本的东西写下来记录下来，算是自己的一个笔记本或日记吧。</p><p>先说下图像吧，一张图像是由像素点组成的，平常我们所看到的彩色图像有三个通道RGB三个也就是三原色通道，而黑白图像就只有一个通道，RGB图像其实就是三层图像叠加在一起组成彩色的图像，一种颜色有256个等级为0~255，如果是灰白图像那就只有一层0-255。一般在图像处理中把图像的像素组成一个矩阵，然后用矩阵的方法来处理图像对图像进行操作。其实图像处理我觉得数学真的很重要特别是线性代数，线性代数就是图像处理的基础其次就是高数，有了高数就可以在图像上试一下各种算法等。</p><p>接下来上代码：<br>这是YALE数据集的可视化</p><pre class="line-numbers language-none"><code class="language-none">import numpy as npimport matplotlib.pyplot as pltfrom scipy import ioimport cv2 as cv#像numpy，matplotlib，scipy，cv2这些包。直接下一个Anaconda就可以#Anaconda里面各种包都有，如果你是新手多百度看看什么是包，这些包有什么作用#遇到不会多百度x&#x3D;io.loadmat(&#39;C:&#x2F;&#x2F;Users&#x2F;&#x2F;dell&#x2F;&#x2F;Desktop&#x2F;&#x2F;Maching_Learning&#x2F;&#x2F;YALE人脸数据集可视化&#x2F;&#x2F;.idea&#x2F;&#x2F;Yale_32x32.mat&#39;)#载入YALE32mat文件的方法，得到的x是一个字典，可以print一下他的shape看一下里面的属性data&#x3D;[]print(x[&#39;fea&#39;].shape)#数据都存在了fea这个键值里面，print出来发现是165*1024说明有165张#图片每张图片都有1024个特征也就是1024个像素点num&#x3D;x[&#39;fea&#39;].shape[0]#得到总共样本的个数165张图片#print(x[&#39;fea&#39;].shape[0])for i in range(num):    img&#x3D;np.array(1024)#先创建一个一行1024列的数组    img&#x3D;x[&#39;fea&#39;][i]#存放每张图片数据    img.shape&#x3D;32,32#把这1x1024的数组reshape成32x32的矩阵    img&#x3D;img.T#矩阵转置    data.append(img)#把处理过大小的图片存到data列表里面data&#x3D;np.array(data)#把列表转成数组矩阵型out&#x3D;[0]*15       #我们想要搞出个15行11列的一整张图先处理行上for i in range(15):  #对每一行进行遍历   out[i]&#x3D;data[i*11]  #换行   for j in range(10):  #对每一列进行遍历       out[i]&#x3D;np.hstack((out[i],data[i*11+j+1]))#将每一次的图片合在一起，np.hstack是行方向的合并矩阵c&#x3D;out[0]     #上面得到了15个一行11列矩阵现在要把这15行都合在一起，np.vstack讲就是列方向上合并for i in range(14):    c&#x3D;np.vstack((c,out[i+1]))print(c.shape)cv.imshow(&quot;out&quot;,c)cv.waitKey(0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这是COIL20数据集的可视化</p><pre class="line-numbers language-none"><code class="language-none">import numpy as npimport matplotlib.pyplot as pltfrom scipy import ioimport cv2 as cvx&#x3D;io.loadmat(&#39;C:&#x2F;&#x2F;Users&#x2F;&#x2F;dell&#x2F;&#x2F;Desktop&#x2F;&#x2F;Maching_Learning&#x2F;&#x2F;COIL20数据集可视化&#x2F;&#x2F;COIL20.mat&#39;)#导入数据集data&#x3D;[]num&#x3D;x[&#39;fea&#39;].shape[0]#得到数据样本的个数#print(x[&#39;fea&#39;].shape[0])for i in range(num):    img&#x3D;np.array(1024)    img&#x3D;x[&#39;fea&#39;][i]    img.shape&#x3D;32,32    img&#x3D;img.T    data.append(img)data&#x3D;np.array(data)out&#x3D;[0]*30for i in range(30):   out[i]&#x3D;data[i*48]   for j in range(47):       out[i]&#x3D;np.hstack((out[i],data[i*48+j+1]))c&#x3D;out[0]for i in range(29):    c&#x3D;np.vstack((c,out[i+1]))print(c.shape)cv.imshow(&quot;out&quot;,c)cv.waitKey(0)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面分别是YALE和COIL20的可视化出来图片<br><img src="https://img-blog.csdnimg.cn/20200204214224334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20200204214234404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"></p>]]></content>
      
      
      <categories>
          
          <category> OpenCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows10下Tensorflow2.0-GPU的安装（基于CUDA10.0+Python3.7+Anaconda）</title>
      <link href="2021/05/01/windows10-xia-tensorflow2-0-gpu-de-an-zhuang-ji-yu-cuda10-0-python3-7-anaconda/"/>
      <url>2021/05/01/windows10-xia-tensorflow2-0-gpu-de-an-zhuang-ji-yu-cuda10-0-python3-7-anaconda/</url>
      
        <content type="html"><![CDATA[<p>Tensorflow是当今深度学习很流行的一个框架，它是由谷歌开发的深度学习框架到现在已经发布到了TF2.0版本了。TF的安装有两个版本一个是CPU版另一个是GPU版。当然GPU上运行TF的速度自然比CPU会快，但是自然它的安装也比CPU版要麻烦。CPU版的TF的安装十分的简单，这里当然不作叙述，本文主要是想记录下自己安装GPU版中遇到的一些问题和坑。</p><p>一、首先安装GPU版本的Tensorflow第一步就是检查自己的电脑配置够格不，在官网有配置要求<a href="https://tensorflow.google.cn/install/gpu">TF官网</a>，我的笔记本电脑显卡是NVIDIA的GTX 1050。如何查看自己电脑显卡配置就是在桌面右击选择NVDIA控制面板然后就可以看到自己的电脑显卡是什么。显卡一定要是NVDIA的AMD的不行，因为在后续要安装CUDA.</p><p>二、安装CUDA10.0.CUDA10.0就在官网上下载即可<a href="https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal">CUDA官网</a>如下图选择<br><img src="https://img-blog.csdnimg.cn/20200206225023985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>下载完成后就会慢慢的出现一个安装CUDA的程序如下图，点击自定义安装(高级)<br><img src="https://img-blog.csdnimg.cn/20200206225250251.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>本人电脑已有VS2017故不再装与VS有关的配置<br>接下来就是安装了，就选择默认路径了不要再去改到其他地方，我看网上说改到其他路径可能会出现问题，当然我没有去试过。<br><img src="https://img-blog.csdnimg.cn/20200206225656147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>然后CUDA10.0就安装完毕了，然后可能你会看到网上还有什么配置环境变量什么的其实最后人家软件都帮你自动配置好了根本就不用在自己配置了</p><p>三、CUDNN的安装。CUDNN同样也是去官网上去下载然后需要保存到CUDA当时的安装路径下。先给出下载地址<a href="https://developer.nvidia.com/rdp/cudnn-archive">CUDNN下载地址</a>这里要选择对应CUDA10.0版本的cudnn。下载好之后呢你要把这个zip文件解压到这里<br><img src="https://img-blog.csdnimg.cn/2020020623024432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>这里的cudnn文件夹是提前自己创建的，然后解压到这里面来。这个CUDA路径就是当时安装CUDA的时候我说的默认路径，如果你选的是默认路径跟着我走就是对的。接下来呢就是需要把解压后的文件夹里的三个小文件复制到CUDA里面。<br><img src="https://img-blog.csdnimg.cn/20200206230527420.png"><br>这是官网的教程说明，应该很显而易懂。（解压后的文件夹名字就是cuda）<br>这里附上官网安装cudnn教程<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-windows">cudnn官方教程</a><br>到这里TF2.0的铺垫安装基本完成了接下来就是TF2.0的安装</p><p>四、Tensorflow2.0-GPU的安装<br>这里我们用国内清华源下载比较快，如果不用的话可能会超级无敌慢</p><pre class="line-numbers language-none"><code class="language-none">pip install tensorflow-gpu&#x3D;&#x3D;2.0.0 -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>接下来就是测试我们的Tensorflow2.0了<br>打开Pycharm（或者是Jupyter Notebook）</p><pre class="line-numbers language-none"><code class="language-none">import tensorflow as tfversion &#x3D; tf.__version__gpu_ok &#x3D; tf.test.is_gpu_available()print(&quot;tf version:&quot;,version,&quot;\nuse GPU&quot;,gpu_ok)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>输出</p><pre class="line-numbers language-none"><code class="language-none">tf version: 2.0.0use GPU True<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>当然这里如何判断是否代码真的在GPU上面跑，执行以下代码</p><pre class="line-numbers language-none"><code class="language-none">import tensorflow.compat.v1 as tftf.disable_v2_behavior()a &#x3D; tf.constant([1.2,2.3,3.6], shape&#x3D;[3],name&#x3D;&#39;a&#39;)b &#x3D; tf.constant([1.2,2.3,3.6], shape&#x3D;[3],name&#x3D;&#39;b&#39;)c &#x3D; a+bsession &#x3D; tf.Session(config&#x3D;tf.ConfigProto(log_device_placement&#x3D;True))print(session.run(c))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出这样就是GPU</p><pre class="line-numbers language-none"><code class="language-none">Device mapping:&#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;gpu:0 -&gt; device: 0, name: Quadro M5000, pci bus id: 0000:01:00.0add: (Add): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;gpu:0b: (Const): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;gpu:0a: (Const): &#x2F;job:localhost&#x2F;replica:0&#x2F;task:0&#x2F;gpu:0[2.4 4.6 7.2]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可能会刷到说用import tensrflow as tf 然后tf.Session（）做测试，但是会报错说没有modelSession那是因为TF到2.0后的一小改动它的Session在tf.compat.v1里面了。</p>]]></content>
      
      
      <categories>
          
          <category> Tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensorflow2 </tag>
            
            <tag> GPU </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>车牌识别：HyperLPR车牌识别代码解析</title>
      <link href="2021/05/01/che-pai-shi-bie-hyperlpr-che-pai-shi-bie-dai-ma-jie-xi/"/>
      <url>2021/05/01/che-pai-shi-bie-hyperlpr-che-pai-shi-bie-dai-ma-jie-xi/</url>
      
        <content type="html"><![CDATA[<p>首先声明，这只是本人自己对HyperLPR代码的看法解析可能会有错还请多多谅解。<br>先贴上HyperLPR源码的链接<a href="https://github.com/zeusees/HyperLPR">HyperLPR</a><br>其中最有用的其实就是HyperLPRLite.py这个代码文件，原来Github上的使用教程可能有点老了不太适用，这边附上一个简单的demo使用，创建一个新的文件夹然后把HyperLPRLite.py放进文件夹，再在新文件夹里面创建一个demo.py文件，如下（记得要把xml，两个h5文件一起放进去）</p><pre class="line-numbers language-none"><code class="language-none">import sysfrom PIL import ImageFontfrom PIL import Imagefrom PIL import ImageDrawimport HyperLPRLite as primport cv2import numpy as npimport timeimport importlibdef visual_draw_position(grr):    max&#x3D;0    model &#x3D; pr.LPR(&quot;model&#x2F;cascade.xml&quot;,&quot;model&#x2F;model12.h5&quot;,&quot;model&#x2F;ocr_plate_all_gru.h5&quot;)    for pstr,confidence,rect in model.SimpleRecognizePlateByE2E(grr):        if 1:            max&#x3D;confidence            grr &#x3D; drawRectBox(grr, rect, pstr+&quot; &quot;+str(round(confidence,3)))            print(&quot;车牌号:&quot;)            print(pstr)            print(&quot;置信度&quot;)            print(confidence)    cv2.imshow(&quot;image&quot;,grr)    cv2.waitKey(0)test_image &#x3D; cv2.imread(&quot;C:&#x2F;&#x2F;Users&#x2F;&#x2F;dell&#x2F;&#x2F;Desktop&#x2F;&#x2F;Cars&#x2F;&#x2F;30.jpg&quot;)visual_draw_position(test_image)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上就可以简单的使用HyperLPR这个开源的车牌识别库</p><p>现在进入正题我们来看一下HyperLPRLite.py这个重要文件里面写的内容<br>接下来我会按照代码执行的顺序来解读各个函数的作用<br>其实这份代码总共分为三大部分：粗定位，精定位，车牌信息识别<br>1、车牌粗定位</p><pre class="line-numbers language-none"><code class="language-none">def computeSafeRegion(self,shape,bounding_rect):    #该函数是初步的框出车牌的大概位置属于粗定位出一个保险的包含车牌的大小    top &#x3D; bounding_rect[1] # y    bottom  &#x3D; bounding_rect[1] + bounding_rect[3] # y +  h    left &#x3D; bounding_rect[0] # x    right &#x3D;   bounding_rect[0] + bounding_rect[2] # x +  w    min_top &#x3D; 0    max_bottom &#x3D; shape[0]    min_left &#x3D; 0    max_right &#x3D; shape[1]    if top &lt; min_top:        top &#x3D; min_top    if left &lt; min_left:        left &#x3D; min_left    if bottom &gt; max_bottom:        bottom &#x3D; max_bottom    if right &gt; max_right:        right &#x3D; max_right    return [left,top,right-left,bottom-top]def cropImage(self,image,rect):    #框出车牌识别的区域    x, y, w, h &#x3D; self.computeSafeRegion(image.shape,rect)    return image[round(y):y+h,round(x):x+w]#返回所有车牌的boxdef detectPlateRough(self,image_gray,resize_h &#x3D; 720,en_scale &#x3D;1.08 ,top_bottom_padding_rate &#x3D; 0.05):    if top_bottom_padding_rate&gt;0.2:        print(&quot;error:top_bottom_padding_rate &gt; 0.2:&quot;,top_bottom_padding_rate)        exit(1)    height &#x3D; image_gray.shape[0]    padding &#x3D;    int(height*top_bottom_padding_rate)    scale &#x3D; image_gray.shape[1]&#x2F;float(image_gray.shape[0])    image &#x3D; cv2.resize(image_gray, (int(scale*resize_h), resize_h))    image_color_cropped &#x3D; image[padding:resize_h-padding,0:image_gray.shape[1]]    image_gray &#x3D; cv2.cvtColor(image_color_cropped,cv2.COLOR_RGB2GRAY)    watches &#x3D; self.watch_cascade.detectMultiScale(image_gray, en_scale, 2, minSize&#x3D;(36, 9),maxSize&#x3D;(36*40, 9*40))    cropped_images &#x3D; []    for (x, y, w, h) in watches:        x -&#x3D; w * 0.14        w +&#x3D; w * 0.28        y -&#x3D; h * 0.15        h +&#x3D; h * 0.3        cropped &#x3D; self.cropImage(image_color_cropped, (int(x), int(y), int(w), int(h)))        cropped_images.append([cropped,[x, y+padding, w, h]])    return cropped_images<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这个地方的重点是detectPlateRough（）函数，首先开始的时候会对图像进行一些插补还有调整图像大小比例操作（其实个人认为这些不是很必要），然后核心的一部就是这个Cascade级联分类器的应用，这才是核心。这里的级联分类器是基于Haar+Adaboost构成的。很重要的就是之前的cascade.xml文件，这个文件里面存放的都是车牌的Haar特征。<br>在这里我们采用了cascade.xml 检测模型——目前效果最好的cascad级联检测模型。然后使用OpenCV的detectMultiscale的方法来对图像进行滑动窗口遍历寻找车牌，实现粗定位。detectMultiscale函数为多尺度多目标检测：多尺度：通常搜索目标的模板尺寸大小是固定的，但是不同图片大小不同，所以目标对象的大小也是不定的，所以多尺度即不断缩放图片大小（缩放到与模板匹配），通过模板滑动窗函数搜索匹配；同一副图片可能在不同尺度下都得到匹配值，所以多尺度检测函数detectMultiscale是多尺度合并的结果。</p><p>简单来讲就是cascade.xml这个文件是通过很多的正样本车牌图片和负样本非车牌图片然后经过一个exe的程序可以创建出一个cascade.xml文件其中文件里Haar特征数据已经过Adaboost处理。最后通过这个xml文件就可以训练出一个级联分类器分类器的判别车牌标准是通过计算好多车牌特征后得出的一个阈值，大于这个阈值判别为车牌。这就是这个函数大概的功能，其中有一些框选图像中车牌的部分的功能比较基础此处不讲。<br><img src="https://img-blog.csdnimg.cn/20200317012359374.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>推荐原作者的讲解：<a href="https://blog.csdn.net/relocy/article/details/78705662/">HyperLPR车牌识别技术算法之车牌粗定位与训练</a></p><p>2、车牌精定位</p><pre class="line-numbers language-none"><code class="language-none">def model_finemapping(self):    input &#x3D; Input(shape&#x3D;[16, 66, 3])  # change this shape to [None,None,3] to enable arbitraty shape input    x &#x3D; Conv2D(10, (3, 3), strides&#x3D;1, padding&#x3D;&#39;valid&#39;, name&#x3D;&#39;conv1&#39;)(input)    x &#x3D; Activation(&quot;relu&quot;, name&#x3D;&#39;relu1&#39;)(x)    x &#x3D; MaxPool2D(pool_size&#x3D;2)(x)    x &#x3D; Conv2D(16, (3, 3), strides&#x3D;1, padding&#x3D;&#39;valid&#39;, name&#x3D;&#39;conv2&#39;)(x)    x &#x3D; Activation(&quot;relu&quot;, name&#x3D;&#39;relu2&#39;)(x)    x &#x3D; Conv2D(32, (3, 3), strides&#x3D;1, padding&#x3D;&#39;valid&#39;, name&#x3D;&#39;conv3&#39;)(x)    x &#x3D; Activation(&quot;relu&quot;, name&#x3D;&#39;relu3&#39;)(x)    x &#x3D; Flatten()(x)    output &#x3D; Dense(2,name &#x3D; &quot;dense&quot;)(x)    output &#x3D; Activation(&quot;relu&quot;, name&#x3D;&#39;relu4&#39;)(output)    model &#x3D; Model([input], [output])    return modeldef finemappingVertical(self,image,rect):    resized &#x3D; cv2.resize(image,(66,16))    resized &#x3D; resized.astype(np.float)&#x2F;255    #cv2.imshow(&quot;wd&quot;, resized)    res_raw&#x3D; self.modelFineMapping.predict(np.array([resized]))[0]#这里会返回两个值    print(self.modelFineMapping.predict(np.array([resized])))    res  &#x3D;res_raw*image.shape[1]    res &#x3D; res.astype(np.int)    H,T &#x3D; res    H-&#x3D;3    if H&lt;0:        H&#x3D;0    T+&#x3D;2;    if T&gt;&#x3D; image.shape[1]-1:        T&#x3D; image.shape[1]-1    rect[2] -&#x3D;  rect[2]*(1-res_raw[1] + res_raw[0])    rect[0]+&#x3D;res[0]    cv2.imshow(&quot;ww&quot;,image)    image &#x3D; image[:,H:T+2]    cv2.imshow(&quot;wd&quot;,image)    image &#x3D; cv2.resize(image, (int(136), int(36)))    return image,rect<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>model12.h5这是这部分左右边界回归模型<br>这里的精定位其实就是切掉原来上面粗定位出来车牌的多余部分，这里用到了一个CNN网络来实现左右边届切除点的预测也就是self.modelFineMapping.predict()这个函数他会返回两个值两个小数值，左右边界分别要切除多少。其实这部分我有点迷，我不太懂如何利用CNN网络来训练出得到两个切除边界的值，因为一般往往CNN网络用来做图像中分类较多，至少对我来说我很少看到CNN拿来做回归。所以在这一部分我不是很看懂。<br>这里附上原作者的：<a href="https://blog.csdn.net/Relocy/article/details/78705566">HyperLPR车牌识别技术算法之车牌精定位</a></p><p>3、车牌信息识别</p><pre class="line-numbers language-none"><code class="language-none">def fastdecode(self,y_pred):    results &#x3D; &quot;&quot;    confidence &#x3D; 0.0    table_pred &#x3D; y_pred.reshape(-1, len(chars)+1)    res &#x3D; table_pred.argmax(axis&#x3D;1)    for i,one in enumerate(res):        if one&lt;len(chars) and (i&#x3D;&#x3D;0 or (one!&#x3D;res[i-1])):            results+&#x3D; chars[one]            confidence+&#x3D;table_pred[i][one]    confidence&#x2F;&#x3D; len(results)    return results,confidencedef model_seq_rec(self,model_path):    width, height, n_len, n_class &#x3D; 164, 48, 7, len(chars)+ 1  #输入层为164*48*3的tensor，类别有len（chars）+1个    rnn_size &#x3D; 256    input_tensor &#x3D; Input((164, 48, 3))    x &#x3D; input_tensor    base_conv &#x3D; 32    for i in range(3):                                    #一共做了三次卷积池化        x &#x3D; Conv2D(base_conv * (2 ** (i)), (3, 3))(x)     #卷积层        x &#x3D; BatchNormalization()(x)                       #统一量纲防止网络失衡        x &#x3D; Activation(&#39;relu&#39;)(x)                         #采用RELU激活函数        x &#x3D; MaxPooling2D(pool_size&#x3D;(2, 2))(x)             #池化层    conv_shape &#x3D; x.get_shape()    x &#x3D; Reshape(target_shape&#x3D;(int(conv_shape[1]), int(conv_shape[2] * conv_shape[3])))(x)    x &#x3D; Dense(32)(x)                                      #全连接层    x &#x3D; BatchNormalization()(x)    x &#x3D; Activation(&#39;relu&#39;)(x)    gru_1 &#x3D; GRU(rnn_size, return_sequences&#x3D;True, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;gru1&#39;)(x)    gru_1b &#x3D; GRU(rnn_size, return_sequences&#x3D;True, go_backwards&#x3D;True, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;gru1_b&#39;)(x)    gru1_merged &#x3D; add([gru_1, gru_1b])    gru_2 &#x3D; GRU(rnn_size, return_sequences&#x3D;True, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;gru2&#39;)(gru1_merged)    gru_2b &#x3D; GRU(rnn_size, return_sequences&#x3D;True, go_backwards&#x3D;True, kernel_initializer&#x3D;&#39;he_normal&#39;, name&#x3D;&#39;gru2_b&#39;)(gru1_merged)    x &#x3D; concatenate([gru_2, gru_2b])    x &#x3D; Dropout(0.25)(x)                                   #正则化，扔掉25%的隐层神经元    x &#x3D; Dense(n_class, kernel_initializer&#x3D;&#39;he_normal&#39;, activation&#x3D;&#39;softmax&#39;)(x)  #再一次全连接，最后使用softmax输出类别数的网络    base_model &#x3D; Model(inputs&#x3D;input_tensor, outputs&#x3D;x)    base_model.load_weights(model_path)    return base_model    def recognizeOne(self,src):    x_tempx &#x3D; src    x_temp &#x3D; cv2.resize(x_tempx,( 164,48))    x_temp &#x3D; x_temp.transpose(1, 0, 2)    y_pred &#x3D; self.modelSeqRec.predict(np.array([x_temp]))    y_pred &#x3D; y_pred[:,2:,:]    return self.fastdecode(y_pred)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此处的车牌字符信息识别事实上是采用了OCR光学字符识别技术也就是在不分割字符的前提下能够识别出车牌一共七个字符。当然传统的车牌字符识别就是先分割字符然后再逐一使用分类算法进行识别。这种方法有个好处就是，仅仅需要较少的字符样本即可用于分类器的训练。在光照，相机条件好的情况下也能取得较好的效果。现在大多数商业车牌识别软件采用的也是这种方法。但是在某些恶劣的自然情况下，车牌字符的分割和识别变得尤其的困难，传统的方法并不能取得很好的结果。这时候我们就能考虑下是否能整体一起识别。当然我们注意到车牌字符的数量是固定的，对于中国车牌而言，车牌一直是7个字符。我们就可以采用多标签分类的方法直接输出多个标签。<br><img src="https://img-blog.csdnimg.cn/20200317011502799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"></p><p>但是这种方法有个很大的缺点，就是需要大量的样本，样本需要涵盖所有的省份和地区。这就为样本的收集造成的极大的困难</p><p>这里采用CNN+GRU的方法来实现<br>GRU：（我个人理解CNN网络是初步的对字符进行分类，而GRU实际是并不常用在分类上，GRU更多用在预测处理上，我个人认为这里运用了双向GRU是为了在车牌图片较为模糊时，也可以大概预测出一个字符类别就是经过G双向GRU后可以更好的预测）<br>GRU(Gated Recurrent Unit) 是由 Cho, et al. (2014) 提出，是LSTM的一种变体。GRU的结构与LSTM很相似，LSTM有三个门，而GRU只有两个门且没有细胞状态，简化了LSTM的结构。而且在许多情况下，GRU与LSTM有同样出色的结果。GRU有更少的参数，因此相对容易训练且过拟合问题要轻一点。<br>下图展示了GRU的网络结构，GRU的网络结构和LSTM的网络结构很相似，LSTM中含有三个门结构和细胞状态，而GRU只有两个门结构：更新门和重置门，分别为图中的z_t和r_t，结构上比LSTM简单。把GRU看着LSTM的变体，相当于取消了LSTM中的cell state，只使用了hidden state,并且使用update gate更新门来替换LSTM中的输入们和遗忘门，取消了LSTM中的输出门，新增了reset gate重置门。这样做的好处是在达到LSTM相近的效果下，GRU参数更少，训练的计算开销更小，训练速度更快。</p><p>这里在CNN的训练模型后加上的是双向GRU<br><img src="https://img-blog.csdnimg.cn/20200317011520269.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>以上就是我对于HyperLPR代码的理解，如有不足之处，请勿喷！蟹蟹！</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 车牌识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>车牌识别：Haar特征+Adaboost原理</title>
      <link href="2021/05/01/che-pai-shi-bie-haar-te-zheng-adaboost-yuan-li/"/>
      <url>2021/05/01/che-pai-shi-bie-haar-te-zheng-adaboost-yuan-li/</url>
      
        <content type="html"><![CDATA[<p>利用OpenCV中的级联分类器训练的Haar+Adaboost分类算法<br>在这里我们采用了cascade.xml 检测模型——目前效果最好的cascad级联检测模型。然后使用OpenCV的detectMultiscale的方法来对图像进行滑动窗口遍历寻找车牌，实现粗定位。detectMultiscale函数为多尺度多目标检测：多尺度：通常搜索目标的模板尺寸大小是固定的，但是不同图片大小不同，所以目标对象的大小也是不定的，所以多尺度即不断缩放图片大小（缩放到与模板匹配），通过模板滑动窗函数搜索匹配；同一副图片可能在不同尺度下都得到匹配值，所以多尺度检测函数detectMultiscale是多尺度合并的结果。</p><p>Haar+Adaboost原理详解：<br>1.Haar特征计算<br>最原始的Haar-like特征在2002年的《A general framework for object detection》提出，它定义了四个基本特征结构，如下A，B，C，D所示，可以将它们理解成为一个窗口，这个窗口将在图像中做步长为1的滑动，最终遍历整个图像。<br><img src="https://img-blog.csdnimg.cn/2020031701205972.png"><br>在基本的四个haar特征基础上，文章《An extended set of Haar-like features for rapid object detection》对其做了扩展，将原来的4个扩展为14个。这些扩展特征主要增加了旋转性，能够提取到更丰富的边缘信息。<br><img src="https://img-blog.csdnimg.cn/20200317012123704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>haar特征就是一些矩形区域，同一个类型的矩形区域不管放大多少，黑白面积比都不变。用一个类型的haar放大N倍，盖住原图像中的目标区域，把haar特征白色区域盖住的像素点的值的和减去该haar特征黑色区域的像素点的值的和得到的结果作为haar特征值。<br>比如如下计算举例：<br><img src="https://img-blog.csdnimg.cn/20200317012157173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>总而言之我给出这样的解释：将上面的任意一个矩形放到车牌区域上，然后，将白色区域的像素和减去黑色区域的像素和，得到的值我们暂且称之为车辆特征值，如果你把这个矩形放到一个非车牌区域，那么计算出的特征值应该和车牌特征值是不一样的，而且越不一样越好，所以这些方块的目的就是把车牌特征量化，以区分车牌和非车牌。<br>在计算Haar特征的过程中还有一个很重要的就是积分图，所谓积分图可以理解为计算加速器。在原来的Haar特征计算公式中如果对于一个矩形小窗口去遍历一整张图像然后计算出特征值，要知道这计算量可能会相当大，因此利用积分图的方法可以大大加速这过程。积分图即只需遍历一次一整张图像，积分图的构造方式是位置（i,j）处的值ii(i,j)是原图像(i,j)左上角方向所有像素的和。这样就可以把原本计算两个矩阵像素和的差值变成三步运算。<br><img src="https://img-blog.csdnimg.cn/20200317012218668.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>2.Adaboost分类<br>Adaboost全称是Adaptive Boosting自适应增强，它是由弱学习联合形成强学习的算法，前提是弱学习的能力是相互补充的不可是能力相类似的弱分类器。<br>对于弱分类器就是基于上述的Haar特征计算得出<br>首先对所有样本图片计算其中一种Haar特征<br>其次统计正负样本的Haar特征均值<br>然后找到介于正负样本均值之间的一个阈值，使用此阈值来区分正负样本<br>最后对所有Haar特征都找到这个最佳阈值，所有特征投票，提高分类效果<br><img src="https://img-blog.csdnimg.cn/20200317012244410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>由上述步骤就可以得到一个初步的弱分类器，接下来就是提高被误判样本的权重，然后再训练得到第二个分类器，然后使用第一二个分类器加权对样本分类并根据结果重新计算权重并训练第三个分类器，以此类推直到第N个分类器的错误率小于所设定的值为止。到此为止就可以完成车牌和非车牌的分类。</p><p>以上就是cascade.xml 检测模型的初步检测原理。之后通过detectMultiscale多尺度检测函数实现车牌的最终定位抓取。（Cascade.xml的创建需要准备大量的车牌图像和非车牌图像然后就可以创建模型）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 车牌识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人小日记：记2020年实习面试有感</title>
      <link href="2021/05/01/ge-ren-xiao-ri-ji-ji-2020-nian-shi-xi-mian-shi-you-gan/"/>
      <url>2021/05/01/ge-ren-xiao-ri-ji-ji-2020-nian-shi-xi-mian-shi-you-gan/</url>
      
        <content type="html"><![CDATA[<p>毫无疑问2020年注定是一个不平凡的年份，这一年真的发生了太多的事情，全球局势动荡特别是在疫情发生的时候，当然这一年我恰好大三到了我抉择到底是读研还是找实习的时候，说实话我真的一点也不想去考研，考研真的又累有残酷而且现在全国的考研人数逐年增长考研已不再是意见容易的事情，所以我对考研真的很不耐烦，但是自己又想读研又想继续深造下去毕竟我觉得在本科期间所接触的所学到的真的不多，不过本科期间至少到目前我觉得我的大学生涯没有任何遗憾我觉得我在我自己看来还算可以至少我明白了很多，学到了很多，成长了很多。我希望能够继续在研究生的路上学下去（其实我更希望被保研但是无奈自己这一学期的成绩直接把我打入谷底，那时候真的特别失望），所以自己真的百感交集。在这个寒假期间，于是乎我心中萌生了去向大厂投简历应聘实习生的想法，本就打着侥幸的心理，我是这么想的如果运气好我还能够过一轮然后参加笔试，运气再好一点可以过了笔试参加面试，说实话我刚开始是想说如果真的成功了也不去毕竟我是要考研所以我就体验一下找工作和面试的感觉，顺便发现一下我自身的不足之处，然后进而改之。</p><p>我选择的招聘岗位都是瞄准清一色的AI岗位也就是计算机视觉，深度学习，机器学习等算法岗位，真的是不看不知道一看吓一跳各个大厂就是牛掰，AI岗的岗位要求是真的高啊，全部清一色要求是研究生或研究生以上的学历，而且岗位需求也是真的高，但我也还是勇敢的去尝试了一下。我记得我投了虎牙直播，斗鱼，网易，腾讯，海康威视，百度，阿里巴巴等等，后来还挺幸运的是除了虎牙，斗鱼，海康威视外其他一轮都过了，也不知道是不是我的简历写的比较优秀（嘻嘻），在过了一轮之后，我开始稍微准备一下笔试，在我去牛客网，Leetcode，赛码网上准备刷题的时候真的是被吓一跳，我应聘岗位的笔试怎么这么难，我就做了一些题就在也不想做下去了，特别是OJ的算法编程题，我真的是很讨厌OJ，我依然清楚记得自己在上算法课的时候的作业，怎么都OJ不过，就这样我就不再去刷题就顺其自然。</p><p>到了笔试那天，真的好难啊，我记得特别清楚的就是百度的笔试题目那真的是叫一个难，不过我也算是发现了自己的很多问题，我真的发现自己好菜，自己学的真的一点也不到位，这也难怪人家要得都是研究生。最后经过了许多场的笔试终于被刷掉了hhhhhh,不过后来只有阿里巴巴面试我了，当时也是有点小激动毕竟自己没有想到自己完全没准备的我能够走到面试那一步。</p><p>在阿里HR和我约好面试时间之后，我真的既紧张又害怕，我怕尴尬我怕自己答不出来，我真的很怕尴尬。不过那一天晚上还是来了，我现在还记得当时他先让我自我介绍一波然后开始让我说说自己做过的一些项目，后面他让我讲讲神经网络的东西，还有深度学习的基本的一些知识，我记得当时他让我把BP和卷积神经网络的基本原理描述给他，因为当时是电话面试所以真的很费劲，不过我当时觉得自己也没有讲好自己的讲述的一个逻辑不是很好，后面他开始问我一些数据结构的知识，这正是我很怕的一个点我真的也很讨厌数据结构但又奈何数据结构是技术岗必不可少的科目，在问完数据结构后又问了我关于编译原理的知识，要我把Python和C的代码编译的原理过程阐述一下，我一下子就懵了，最后面试到这里差不多就结束了，然后最后他给我现场布置了一道编程题需要我马上写完然后发到他给我发的邮箱上面，当然那一题编程题还不算难，我很快就写完，至此我的面试就结束了总共花费了一个半小时还多一些，结束后我顿时感觉口干舌燥，我的心总算是放下来了。</p><p>不过后来在面试结束后我认真的反思了自己，发现自己真的很多很多的不足啊，我发现自己对知识的研究还不够透彻还只停留在表层会用但不知道深入的原理。说实话这次面试给我带来的东西和让我学到的东西很多，真的说实话就好像做了一场梦在梦中突然惊醒一般。所以研究生真的很有必要，研究生的目标就是更透彻更深入的去了解学习知识。<br>希望自己在这一次次的经历后能够更好吧！</p>]]></content>
      
      
      <categories>
          
          <category> 个人日记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM学习：Solidworks2018导出小车URDF模型下遇到的问题</title>
      <link href="2021/05/01/slam-xue-xi-solidworks2018-dao-chu-xiao-che-urdf-mo-xing-xia-yu-dao-de-wen-ti/"/>
      <url>2021/05/01/slam-xue-xi-solidworks2018-dao-chu-xiao-che-urdf-mo-xing-xia-yu-dao-de-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这是我在用Solidworks2018来自己画一个autolabor小车装配体然后转成urdf文件然后在ROS下显示出模型时遇到的问题，下面是我的装配体模型。<br><img src="https://img-blog.csdnimg.cn/20200510010801559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>一、问题</strong><br>1、转成urdf模型的步骤我不想多说自己可以看古月居的视频或者百度上面自己搜索或者官网上教程这很简单不是很难。在导出后得到一个功能包，然后把功能包转移到Ubuntu下的ROS工作空间里面的src文件夹下，之后进行catkin_make等基本操作，但当我运行display.launch文件的时候出现了以下情况<br><img src="https://img-blog.csdnimg.cn/20200510011044908.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>这个情况纯属是我自己的粗心导致可以看到很明显的error提示，我有一个joint重复了，后来发现是因为我在soildworks里面命名的joint确实命名重复了，这个得返回solidworks里面重新配置名字然后导出新的功能包就好了</p><p>2、这个问题算是比较麻烦也是不那么好解决的，在解决上面的问题之后本以为已经可以正常显示模型了但是没有想到的是模型出现了下图这样的情况——四个轮子只有一个是正常的<br><img src="https://img-blog.csdnimg.cn/20200510011307824.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>经过不断的查找资料，排除各种情况的操作下，终于发现了问题所在，竟是由于solidworks2018转urdf的插件问题，就是不知道为什么如果在你的装配体中如果出现相同的多个零件，那么在转urdf的时候自动配置会出现异常——即仅会保留相同零件中的一个零件。既然问题已经知道了，那么其实很好解决那就是回到solidworks下重新画四个不一样的轮子然后装配上去即可，至此解决了问题。<br><img src="https://img-blog.csdnimg.cn/20200510011606658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>3、通过以上的经历，感觉差不多了，但是还是有小毛病就是，原来在solidworks里面已经配色好的模型但是在urdf里面颜色已经全部丧失，后来我分析了一波，我个人认为是urdf中的rgb（0-1）颜色范围和我们平时所定义的rgb（0-255）范围不是一致因此才有这样情况，这也好解决只要进入urdf文件夹下去配置一下color这个属性就可以，至于rgba的值大家可以在网上查到资料</p><p><strong>二、结语和总结</strong><br>在整个过程中利用solidworks进行模型构建能够更加真实的仿真模拟，相比于自己用xml语言一个字一个字去敲真的方便太多了，但是这其中难免会有一些困难等。</p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> URDF </tag>
            
            <tag> Solidworks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM学习：使用move_base无法动态实现实时避障</title>
      <link href="2021/05/01/slam-xue-xi-shi-yong-move-base-wu-fa-dong-tai-shi-xian-shi-shi-bi-zhang/"/>
      <url>2021/05/01/slam-xue-xi-shi-yong-move-base-wu-fa-dong-tai-shi-xian-shi-shi-bi-zhang/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先我使用的机器人模型是自己用solidworks制作然后转出为urdf模型的，然后在我用AMCL+Move_base算法的时候发现机器人无法躲避实时障碍物，也就是当你突然放一个物体在机器人面前，激光雷达是可以扫描到前方有物体而且会在rviz中的显示红色的点云信息但是却没有膨胀区域（也就是Move_base功能包中所设定的一个安全+可能危险+危险区域蓝色的一块）</p><p><strong>一、问题描述</strong><br>首先呈上问题的图片<br><img src="https://img-blog.csdnimg.cn/20200521102247545.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>可以看到上面的情况，在小车的面前有个正方体，但是却无法描述它的膨胀区域，最刚开始的时候我一直以为是我的move_base功能包出现了问题然后我就重新的去安装一遍功能包然后发现还是没有用，但是很奇怪的是当我用别人的一个小车模型但是同样的move_base包的时候却意外的发现是可以出现膨胀区域，那这下子问题就是我的小车模型上了。</p><p><strong>二、问题解决</strong><br>在我努力排除可能问题的努力下，终于发现问题所在，那就是我的激光雷达所设置的一个水平高度比较高，然后又因为move_base的配置文件costmap_common_params.yaml里面有一个设置障碍物最小和最大高度的选项；很巧的是两个错开了，就是雷达所在高度扫描过去是可以扫描到但是我move_base配置文件里面设置的识别到的障碍物高度如果在那个范围就直接过滤掉，所以最终原因就是costmap_common_params.yaml中的max_obstacle_height和min_obstacle_height参数设置问题，一般的min_obstacle_height设置为地面高度也就是0,max_obstacle_height设置成高于机器人的高度（我就是设置太低了引发的问题），至此就可以解决问题。<br>这是修改之后的图片<br><img src="https://img-blog.csdnimg.cn/20200521104914602.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>三、总结</strong><br>这里要推荐给大家的参考网站<br><a href="http://wiki.ros.org/move_base/">ROS_wiki</a><br><a href="https://wiki.ros.org/costmap_2d">Costmap_2d</a><br><a href="https://www.ncnynl.com/archives/201708/1920.html">obstacle层参数配置</a></p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM学习：Gmapping建图下地图变形更新速度慢问题</title>
      <link href="2021/05/01/slam-xue-xi-gmapping-jian-tu-xia-di-tu-bian-xing-geng-xin-su-du-man-wen-ti/"/>
      <url>2021/05/01/slam-xue-xi-gmapping-jian-tu-xia-di-tu-bian-xing-geng-xin-su-du-man-wen-ti/</url>
      
        <content type="html"><![CDATA[<p><strong>一、问题描述</strong><br>首先，我在用Gmapping建图的时候发现扫描出来的地图更新速度很慢然后还有就是地图的完整性不高容易变形扭曲，我们在一般在路径规划前需要用适当的建图算法把地图创建出来然后保存方可进行全局路径规划，但是如果地图创建出来扭曲完整性不高那么在后面的路径规划中也更谈不上规划，问题如下图所示<br><img src="https://img-blog.csdnimg.cn/20200521141320563.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>二、问题解决</strong><br>其实主要是修改Gmapping的配置文件，因为官方的配置文件不可以直接用一定要视具体情况而作出修改<br>在gmapping.launch文件中修改</p><pre class="line-numbers language-none"><code class="language-none">&lt;launch&gt;    &lt;arg name&#x3D;&quot;scan_topic&quot; default&#x3D;&quot;scan&quot; &#x2F;&gt;    &lt;node pkg&#x3D;&quot;gmapping&quot; type&#x3D;&quot;slam_gmapping&quot; name&#x3D;&quot;slam_gmapping&quot; output&#x3D;&quot;screen&quot; clear_params&#x3D;&quot;true&quot;&gt;        &lt;param name&#x3D;&quot;odom_frame&quot; value&#x3D;&quot;odom&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;map_update_interval&quot; value&#x3D;&quot;0.1&quot;&#x2F;&gt;   &lt;!--雷达更新速度--&gt;        &lt;!-- Set maxUrange &lt; actual maximum range of the Laser --&gt;        &lt;param name&#x3D;&quot;maxRange&quot; value&#x3D;&quot;12.0&quot;&#x2F;&gt;             &lt;param name&#x3D;&quot;maxUrange&quot; value&#x3D;&quot;10.0&quot;&#x2F;&gt;         &lt;!--雷达扫描最大距离--&gt;&gt;        &lt;param name&#x3D;&quot;sigma&quot; value&#x3D;&quot;0.05&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;kernelSize&quot; value&#x3D;&quot;1&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;lstep&quot; value&#x3D;&quot;0.05&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;astep&quot; value&#x3D;&quot;0.05&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;iterations&quot; value&#x3D;&quot;5&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;lsigma&quot; value&#x3D;&quot;0.075&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;ogain&quot; value&#x3D;&quot;3.0&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;lskip&quot; value&#x3D;&quot;0&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;srr&quot; value&#x3D;&quot;0.01&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;srt&quot; value&#x3D;&quot;0.02&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;str&quot; value&#x3D;&quot;0.01&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;stt&quot; value&#x3D;&quot;0.02&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;linearUpdate&quot; value&#x3D;&quot;0.05&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;angularUpdate&quot; value&#x3D;&quot;0.436&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;temporalUpdate&quot; value&#x3D;&quot;-1.0&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;resampleThreshold&quot; value&#x3D;&quot;0.5&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;particles&quot; value&#x3D;&quot;80&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;xmin&quot; value&#x3D;&quot;-1.0&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;ymin&quot; value&#x3D;&quot;-1.0&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;xmax&quot; value&#x3D;&quot;1.0&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;ymax&quot; value&#x3D;&quot;1.0&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;delta&quot; value&#x3D;&quot;0.05&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;llsamplerange&quot; value&#x3D;&quot;0.01&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;llsamplestep&quot; value&#x3D;&quot;0.01&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;lasamplerange&quot; value&#x3D;&quot;0.005&quot;&#x2F;&gt;        &lt;param name&#x3D;&quot;lasamplestep&quot; value&#x3D;&quot;0.005&quot;&#x2F;&gt;        &lt;remap from&#x3D;&quot;scan&quot; to&#x3D;&quot;$(arg scan_topic)&quot;&#x2F;&gt;    &lt;&#x2F;node&gt;&lt;&#x2F;launch&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的内容主要是修改那两个注释的地方，最主要的就是更新速度，因为更新速度一提升，建图的完整性也会跟着提高很多，下面是做过修改后的地图<br><img src="https://img-blog.csdnimg.cn/20200521142021140.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>三、总结</strong><br>给出一些参考的文献和文章<br><a href="https://blog.csdn.net/qq_42263553/article/details/100587024">参考文献</a></p>]]></content>
      
      
      <categories>
          
          <category> 建图算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> 建图算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM学习：DWA算法原理和Python编程实现</title>
      <link href="2021/05/01/slam-xue-xi-dwa-suan-fa-yuan-li-he-python-bian-cheng-shi-xian/"/>
      <url>2021/05/01/slam-xue-xi-dwa-suan-fa-yuan-li-he-python-bian-cheng-shi-xian/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>此部分内容是ROS中Move_Base功能包用到的DWA路径规划算法的介绍和实现，下面我将以自己所理解的DWA算法原理内容展示出来，我看过网上和书籍很多资料，它们的描述基本大同小异，对于想要清楚了解或刚接触DWA算法的人来讲真的比较难理解可能要多看那么四五遍或五六遍才能真正理解这也是比较耗费时间的，所以本文在我理解的基础上写的，本文内容如果有不当之处还请谅解，如果有错误之处也请指出，毕竟本文是以我自己所理解的方式去完成的。</p><p><strong>一、DWA算法原理</strong><br>首先先给出DWA算法的原理流程图</p><p><img src="https://img-blog.csdnimg.cn/20200621101602368.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"></p><p><strong>首先</strong>我将先讲一讲评价函数的计算，评价函数其实是比较简单的。<br>1、距离目标点评价函数：距离目标点评价函数这里采用的就是很普遍也很普通的欧式距离算法，但具体是哪两个点的距离呢，目标点是人为一开始给定的，我们已经有了，另一个点就是一个速度的模拟轨迹的最后一个位置，可能看到这里还会比较模糊什么是模拟轨迹的最后一个位置，没关系，这里先说一下评价函数要求对评价函数怎么算心中有个概念即可。<br>2、速度评价函数：速度评价函数也比较简单，就是计算刚开始人为设定的机器最大线速度值和模拟轨迹的最后一个位置的线速度的差值，总而言之就是两个速度差值就是，这里也是一样先有个概念。<br>3、轨迹距离障碍物的评价函数：轨迹距离障碍物也比较简单，就是对一条轨迹上每个采样的位置进行循环，再对每个障碍物进行循环，也就是一条模拟轨迹路线上每个位置和每个障碍物之间的欧氏距离如何，然后比较得出一个距离最小的值，当然在循环的过程中要不断判断每个距离是不是小于机器人自身半径，因为如果小于自身半径我们判定机器人已经撞上去了这时候就直接返回。</p><p><strong>其次</strong>，我们要开始讲一下速度采样的问题了，这里我要偷一张图来帮助理解了。<br><img src="https://img-blog.csdnimg.cn/20200621103159659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>蓝色圆圈代表机器人，比如现在机器人正处于如上位置，速度采样是什么意思呢，就是在一个动态范围内选取速度，如上是不是有很多发散出来的线，那就是选取到的速度，这里速度其实就是线速度和角速度组合，那这个速度是要怎么选取呢，其实很简单，那就是规定线速度范围和角速度的范围，然后循环使得线速度和角速度互相匹配组成一个速度。而线速度一开始会设定Vmin——Vmax，然后在实际情况中会有Vc-va<em>dt——Vc+va</em>dt（其中Vc是当前线速度，比如刚开始线速度是0后面会随着速度取样慢慢改变，可能这里讲的有些不明白但是看了代码就一定会懂，va是设定的线速度加速度，dt是间隔时间，也就是一条轨迹上隔多久算一次位置等信息直到模拟轨迹时间）；同理角速度也是如此，经过上述后我们得到两个范围Vmin——Vmax和Vc-va·dt——Vc+va·dt这里我们就要取两个范围的交集了，那肯定是两个范围的最小值的取最大，两个范围的最大值取最小，从而形成一个新的范围，不知道这里我有没有讲清楚。</p><p><strong>之后</strong>速度采样完后我们就可以开始计算模拟轨迹上各个点的位置等相关信息了。<br>位置的计算其实很简单就是如下图计算公式：<br><img src="https://img-blog.csdnimg.cn/2020062110452552.png"><br>这里的v就是线速度，w就是角速度，这里可能很多人会好奇为什么这里计算位置使用直线方式计算，这是因为我们一条模拟轨迹上所设定的时间间隔很小所以可以近似把它看作直线来计算，同时这也使得代码计算更为方便快捷，那这里我再偷一张图。<br><img src="https://img-blog.csdnimg.cn/20200621105935889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>这里可能大家会看到机器人还有个Yrobot方向速度，大家其实可以不用管，因为一般正常的轮子都是只能实现前后方向的位置改变，但是可能有些人会接触到麦克纳姆轮也就是全向移动轮，它可以做到前后左右位置改变，所以其实这里我们大可不必考虑Y方向速度，如果你真想知道你就在百度上随便搜一篇DWA算法你就会在里面找到相关计算方法，但是这里我并不想阐述计算方法。</p><p><strong>最后</strong>，讲到这里DWA算法基本的东西其实已经全部讲完了，其实这个算法并不会太难，如果看不懂就多读几遍，最后的步骤就是不断地循环循环找最优轨迹并沿着最优轨迹走然后再速度采样再模拟轨迹再选择最优轨迹走，直到到达某个位置和目标点的距离不超过自己设定的范围为止。对了，刚刚上面只讲了评价函数但是我给忘了讲评价分的计算，其实也很简单，最简单的方法就是把三个评价函数计算出来的结果全部相加起来当然这是最简单的计算方法，实际上我们不能这样，因为每个参量有大有小，可能因为某个评价函数的结果过大直接覆盖了其他两个，所以我们还是要进行平滑处理的操作，这里我要再去偷个公式图。<br><img src="https://img-blog.csdnimg.cn/20200621105953621.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>这个图里面的head就是距离目标点距离的评价值，dist就是距离障碍物的，velocity就是速度，其实总而言之就是归一化呗，没什么特别深奥的东西，最后的评价分计算公式就是把这归一化后的三个值相加起来，当然可以相应的在每个评价值前面乘上权重系数，也就是偏重于哪个评价值，最后计算公式就是<br><img src="https://img-blog.csdnimg.cn/20200621110256507.png"></p><p><strong>二、DWA算法Python实现代码</strong><br>首先说明一下这个代码我是参考给出的标准的DWA算法实现，但是我是在自己完全理解后自己看着DWA公式来写出算法，代码的每一步基本都有注释。<br>这份代码里面可能大家看到，我没有用权重系数和归一化，这是因为我是在坐标轴上一个小区域直接模拟，所以计算量都不是很大，所以我就没有用到，在真正以后实际情况里，还是要按照公式来做</p><pre class="line-numbers language-none"><code class="language-none">import numpy as npfrom math import *import matplotlib.pyplot as plt#参数设置V_Min &#x3D; -0.5            #最小速度V_Max &#x3D; 3.0             #最大速度W_Min &#x3D; -50*pi&#x2F;180.0    #最小角速度W_Max &#x3D; 50*pi&#x2F;180.0     #最大角速度Va &#x3D; 0.5                #加速度Wa &#x3D; 30.0*pi&#x2F;180.0      #角加速度Vreso &#x3D; 0.01            #速度分辨率Wreso &#x3D; 0.1*pi&#x2F;180.0    #角速度分辨率radius &#x3D; 1              #机器人模型半径Dt &#x3D; 0.1                #时间间隔Predict_Time &#x3D; 4.0      #模拟轨迹的持续时间alpha &#x3D; 1.0             #距离目标点的评价函数的权重系数Belta &#x3D; 1.0             #速度评价函数的权重系数Gamma &#x3D; 1.0             #距离障碍物距离的评价函数的权重系数#障碍物Obstacle&#x3D;np.array([[0,10],[2,10],[4,10],[6,10],[3,5],[4,5],[5,5],[6,5],[7,5],[8,5],[10,7],[10,9],[10,11],[10,13]])#Obstacle &#x3D; np.array([[0, 2]])#距离目标点的评价函数def Goal_Cost(Goal,Pos):    return sqrt((Pos[-1,0]-Goal[0])**2+(Pos[-1,1]-Goal[1])**2)#速度评价函数def Velocity_Cost(Pos):    return V_Max-Pos[-1,3]#距离障碍物距离的评价函数def Obstacle_Cost(Pos,Obstacle):    MinDistance &#x3D; float(&#39;Inf&#39;)          #初始化时候机器人周围无障碍物所以最小距离设为无穷    for i in range(len(Pos)):           #对每一个位置点循环        for j in range(len(Obstacle)):  #对每一个障碍物循环            Current_Distance &#x3D; sqrt((Pos[i,0]-Obstacle[j,0])**2+(Pos[i,1]-Obstacle[j,1])**2)  #求出每个点和每个障碍物距离            if Current_Distance &lt; radius:            #如果小于机器人自身的半径那肯定撞到障碍物了返回的评价值自然为无穷                return float(&#39;Inf&#39;)            if Current_Distance &lt; MinDistance:                MinDistance&#x3D;Current_Distance         #得到点和障碍物距离的最小    return 1&#x2F;MinDistance#速度采用def V_Range(X):    Vmin_Actual &#x3D; X[3]-Va*Dt          #实际在dt时间间隔内的最小速度    Vmax_actual &#x3D; X[3]+Va*Dt          #实际载dt时间间隔内的最大速度    Wmin_Actual &#x3D; X[4]-Wa*Dt          #实际在dt时间间隔内的最小角速度    Wmax_Actual &#x3D; X[4]+Wa*Dt          #实际在dt时间间隔内的最大角速度    VW &#x3D; [max(V_Min,Vmin_Actual),min(V_Max,Vmax_actual),max(W_Min,Wmin_Actual),min(W_Max,Wmax_Actual)]  #因为前面本身定义了机器人最小最大速度所以这里取交集    return VW#一条模拟轨迹路线中的位置，速度计算def Motion(X,u,dt):    X[0]+&#x3D;u[0]*dt*cos(X[2])           #x方向上位置    X[1]+&#x3D;u[0]*dt*sin(X[2])           #y方向上位置    X[2]+&#x3D;u[1]*dt                     #角度变换    X[3]&#x3D;u[0]                         #速度    X[4]&#x3D;u[1]                         #角速度    return X#一条模拟轨迹的完整计算def Calculate_Traj(X,u):    Traj&#x3D;np.array(X)    Xnew&#x3D;np.array(X)    time&#x3D;0    while time &lt;&#x3D;Predict_Time:        #一条模拟轨迹时间        Xnew&#x3D;Motion(Xnew,u,Dt)        Traj&#x3D;np.vstack((Traj,Xnew))   #一条完整模拟轨迹中所有信息集合成一个矩阵        time&#x3D;time+Dt    return Traj#DWA核心计算def dwa_Core(X,u,goal,obstacles):    vw&#x3D;V_Range(X)    best_traj&#x3D;np.array(X)    min_score&#x3D;10000.0                 #随便设置一下初始的最小评价分数    for v in np.arange(vw[0], vw[1], Vreso):         #对每一个线速度循环        for w in np.arange(vw[2], vw[3], Wreso):     #对每一个角速度循环            traj&#x3D;Calculate_Traj(X,[v,w])            goal_score&#x3D;Goal_Cost(goal,traj)            vel_score&#x3D;Velocity_Cost(traj)            obs_score&#x3D;Obstacle_Cost(traj,Obstacle)            score&#x3D;goal_score+vel_score+obs_score            if min_score&gt;&#x3D;score:                    #得出最优评分和轨迹                min_score&#x3D;score                u&#x3D;np.array([v,w])                best_traj&#x3D;traj    return u,best_trajx&#x3D;np.array([2,2,45*pi&#x2F;180,0,0])                          #设定初始位置，角速度，线速度u&#x3D;np.array([0,0])                                        #设定初始速度goal&#x3D;np.array([8,8])                                     #设定目标位置global_tarj&#x3D;np.array(x)for i in range(1000):                                     #循环1000次，这里可以直接改成while的直到循环到目标位置    u,current&#x3D;dwa_Core(x,u,goal,Obstacle)    x&#x3D;Motion(x,u,Dt)    global_tarj&#x3D;np.vstack((global_tarj,x))                 #存储最优轨迹为一个矩阵形式每一行存放每一条最有轨迹的信息    if sqrt((x[0]-goal[0])**2+(x[1]-goal[1])**2)&lt;&#x3D;radius:  #判断是否到达目标点        print(&#39;Arrived&#39;)        breakplt.plot(global_tarj[:,0],global_tarj[:,1],&#39;*r&#39;,Obstacle[0:3,0],Obstacle[0:3,1],&#39;-g&#39;,Obstacle[4:9,0],Obstacle[4:9,1],&#39;-g&#39;,Obstacle[10:13,0],Obstacle[10:13,1],&#39;-g&#39;)  #画出最优轨迹的路线plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>效果展示<br><img src="https://img-blog.csdnimg.cn/20200621195652606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>三、结语和总结</strong><br>其实看不懂的东西只要多看几遍多耐心的琢磨就肯定会明白其中的道理，多看几遍真的没有错。只要自己不要自暴自弃就好，以前老师总说不懂的知识多看几遍，我一直不放在心上，长大后才发现老师说的真的是对的，很多事情也是如此，多看多做几遍，简单的事情重复做，重复的事情坚持做！</p><p><strong>四、参考文献</strong><br><a href="https://blog.csdn.net/u011600592/article/details/54613717">https://blog.csdn.net/u011600592/article/details/54613717</a><br><a href="https://scm_mos.gitlab.io/motion-planner/dwa-local-planner/">https://scm_mos.gitlab.io/motion-planner/dwa-local-planner/</a></p>]]></content>
      
      
      <categories>
          
          <category> 路径规划 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 路径规划算法 </tag>
            
            <tag> SLAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM学习：记录配置摄像头时候出现的一些小问题</title>
      <link href="2021/05/01/slam-xue-xi-ji-lu-pei-zhi-she-xiang-tou-shi-hou-chu-xian-de-yi-xie-xiao-wen-ti/"/>
      <url>2021/05/01/slam-xue-xi-ji-lu-pei-zhi-she-xiang-tou-shi-hou-chu-xian-de-yi-xie-xiao-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文是关于笔者在为自己建立的模型上添加摄像头以便为了后续的视觉开发做准备，在配置摄像头的时候遇到了一些小问题，这里本文也只是为了记录而已，可能文章略简，请谅解（本文摄像头使用的是Kinect摄像头）</p><p><strong>一、问题一描述</strong><br><img src="https://img-blog.csdnimg.cn/20200622001437447.png"><br>在配置好一切摄像头应该配置的文件之后，运行在gazebo中显示模型的launch文件，出现了以上错误。<br>解决方法：<br>1、可能是urdf文件书写有错误，要注意urdf文件中不能加中文注释，第一行中不能加注释，打开urdf文件删掉相关注释即可。<br>2、安装joint-state-publisher-gui</p><pre class="line-numbers language-none"><code class="language-none">sudo apt-get install ros-kinetic-joint-state-publisher-gui<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>二、问题二描述</strong><br><img src="https://img-blog.csdnimg.cn/20200622001915754.png"><br>在一个自己已经搭建好的world中加载我的机器人模型，发现爆出警告机器人模型已存在，并且显示出来的机器人模型是旧版的不带摄像头的模型。<br>原因：因为我在搭建模型的时候是在机器人模型已经存在在空世界的环境下搭建所以创建出来的world自带了model所以在构建world的时候一定要把模型删去。<br>解决方法：<br>重新在无模型的空世界下搭建world并保存。</p><p><strong>三、参考文献</strong><br><a href="https://blog.csdn.net/qq_40081208/article/details/101533221">https://blog.csdn.net/qq_40081208/article/details/101533221</a><br><a href="https://blog.csdn.net/liuyuekelejic/article/details/105476063">https://blog.csdn.net/liuyuekelejic/article/details/105476063</a></p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> ROS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SLAM学习：JetsonTX2工控机的准备工作</title>
      <link href="2021/05/01/slam-xue-xi-jetsontx2-gong-kong-ji-de-zhun-bei-gong-zuo/"/>
      <url>2021/05/01/slam-xue-xi-jetsontx2-gong-kong-ji-de-zhun-bei-gong-zuo/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>笔者已在自己的PC机上的Ubuntu系统上已经全部模拟仿真好Autolabor无人小车实现SLAM，视觉目标检测等功能后，准备把所有的源码移植到在JetsonTX2上面，然后利用TX2连接Autolabor小车来实现真正的无人驾驶功能，接下来这是我在使用TX2的时候记录一些比较麻烦的问题，全篇内容较短。</p><p><strong>一、TX2刷机成Ubuntu16.04LTS系统</strong><br>首先我们得明确我们是要在TX2上面用ROS-Kinetic那就必须安装Ubuntu16.04,但是现在已经2020年，Jetpack的版本都更新到哪里去了，现在基本都是支持Ubuntu18.04,但Ubuntu16.04也有只不过不好搞，这里分享一篇我一直参考比较多的一片博客，写的还不错。<br><a href="https://blog.csdn.net/DeepWolf/article/details/88640937">TX2刷机（Jetpack3.3）</a><br>当然刷机的时候会遇到问题（比如Jetpack刚开的时候是空白），这一篇博客已经写了我要说的东西了。<br><a href="https://blog.csdn.net/bluewhalerobot/article/details/81587039">TX2刷机常见问题</a><br>这里我额外说一句，主机上安装Jetpack的时候你的系统的语言你得改为EN也就是英语，而且后面Jetpack保存路径也要是英文的不要中文否则会出错哦。<br><a href="https://blog.csdn.net/BobYuan888/article/details/88662779">Ubuntu修改系统语言</a></p><p><strong>二、TX2必须装上CUDA和Cudnn</strong><br>首先说明，一定要看清楚JetsonTX2是ARMV8结构也就是Arm64-bit不是AMD64啊，不要乱下载，如果在刷机的时候你的CUDA和Cudnn没安装上没关系，到了TX2上面手动下载<br><a href="https://blog.csdn.net/sunshinefcx/article/details/104024074?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-5.nonecase">TX2安装CUDA和CUdnn</a></p><p><strong>三、TX2的Ubuntu不一样</strong><br>由于TX2的Ubuntu16.04是建立在它ArmV8架构上，所以有很多地方不一样要小心，比较重要的一个就是当你TX2有了界面后登录进去第一件事一定是换镜像源，不然后面安装一些其他任何软件等会出问题，原因大概是因为arm架构我记得，反正第一件事要换源，至于怎么换源请看<br><a href="https://blog.csdn.net/mathlxj/article/details/99626029">TX2换镜像源</a></p><p><strong>四、结语和总结</strong><br>不管是TX2也好还是其他嵌入式开发平台也好，一定要清楚机子的配置，因为我们平常都用惯了x86/x64的处理器很可能会习惯性的在新的嵌入式板上做错误的操作。</p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> ROS </tag>
            
            <tag> NVIDIA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git学习:Git命令把项目文件上传到Github</title>
      <link href="2021/05/01/git-xue-xi-git-ming-ling-ba-xiang-mu-wen-jian-shang-chuan-dao-github/"/>
      <url>2021/05/01/git-xue-xi-git-ming-ling-ba-xiang-mu-wen-jian-shang-chuan-dao-github/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>记录使用Git命令上传工程项目到个人Github上，怕自己以后会忘记，操作系统是在Windows上进行，学会Windows操作，Linux上自然会，都差不多。</p><p><strong>一、下载Git</strong><br>百度搜索Git，到官网上下载Git对应版本型号<br><strong>二、在本地的Github个人中心创建仓库</strong><br><img src="https://img-blog.csdnimg.cn/20200721230532710.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>三、配置SSH</strong><br>1、在需要上传的工程文件夹下面右键点击Git bash here输入命令</p><pre class="line-numbers language-none"><code class="language-none">ssh-keygen -t rsa -C  &quot;邮箱&quot; <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>2、进入SSH密钥存放地址我的电脑——C盘——用户——.ssh/文件夹，打开id_ras.pub文件，如果打不开就用记事本打开，然后复制里面的所有内容<br>3、点击New SSH Key然后创建好后将复制的内容粘贴到Github的SSH密钥栏处<br><img src="https://img-blog.csdnimg.cn/20200721231037327.png"><br><img src="https://img-blog.csdnimg.cn/20200721231129993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>4、检查密钥是否配置成功，在刚刚的Git bash here的命令行下面输入</p><pre class="line-numbers language-none"><code class="language-none">ssh -T git@github.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>四、上传工程文件</strong><br>这些操作都是在要上传的工程文件夹下面，就在工程文件的根目录下就行<br>1、配置账号名和邮箱</p><pre class="line-numbers language-none"><code class="language-none">git config --global user.email &quot;邮箱&quot;git config --global user.name ‘名字’<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>2、初始化Git空间</p><pre class="line-numbers language-none"><code class="language-none">git init<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>3、上传文件到暂存区（如果上传工程目录下所有文件就是add .如果上传某个文件就是add xxxx.xx）</p><pre class="line-numbers language-none"><code class="language-none">git add .<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>4、暂存区的文件上传到Git仓库空间</p><pre class="line-numbers language-none"><code class="language-none">git commit -m &quot;此处输入项目注释语句&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>5、建立Github仓库和工程文件夹下Git的联系（xxxxxxxx是指Github建立仓库的SSH地址）<br><img src="https://img-blog.csdnimg.cn/20200721231902883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"></p><pre class="line-numbers language-none"><code class="language-none">git remote add origin xxxxxxxxxxxx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>6、克隆Readme文档到本低目录下</p><pre class="line-numbers language-none"><code class="language-none">git pull --rebase origin master<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>7、上传文件到Github上</p><pre class="line-numbers language-none"><code class="language-none">git push -u origin master<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>五、参考文献</strong><br><img src="https://img-blog.csdnimg.cn/20200930163208745.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70#pic_center"></p><p><a href="https://www.cnblogs.com/Amywj/p/13219108.html">使用Git上传工程项目</a></p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu16.04下Clash配置出现的问题和记录</title>
      <link href="2021/05/01/ubuntu16-04-xia-clash-pei-zhi-chu-xian-de-wen-ti-he-ji-lu/"/>
      <url>2021/05/01/ubuntu16-04-xia-clash-pei-zhi-chu-xian-de-wen-ti-he-ji-lu/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文是关于在Linux下Clash时候出现的一些问题，写这篇博客仅此记录<br><strong>一、更改了连接密码后怎么办</strong><br>如果在网址端修改了连接密码，那么相应的UUID号也会跟着改变，所以相应的配置文件也同样需要改变，Linux下配置文件就是config.yaml这是最主要的文件;在改变连接密码后，把原来文件夹下的config.yaml文件删除掉，然后先运行一下程序后再关闭则会出现一个不完整的config文件然后关闭，再输入下面的代码，然后刷新文件夹即可生成一个新的config文件，然后再和原来一样运行回去就可以解决</p><pre class="line-numbers language-none"><code class="language-none">sudo curl 网址 &gt;&gt; config.yaml<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>二、参考文献</strong><br><a href="https://blog.csdn.net/qq_27036771/article/details/106748409?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase">https://blog.csdn.net/qq_27036771/article/details/106748409?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase</a></p>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> Clash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git学习：Readme文档的编写</title>
      <link href="2021/05/01/git-xue-xi-readme-wen-dang-de-bian-xie/"/>
      <url>2021/05/01/git-xue-xi-readme-wen-dang-de-bian-xie/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>该文是关于在Github下创建好仓库，并且将项目文件上传到仓库后，如何去编写好一份优秀的Readme文件，Readme文件的编写使用Markdown编辑模式撰写，和CSDN官方博客撰写其实差不多，所以会了CSDN的Markdown撰写其实差不多Readme也会，只不过两者可能在操作上或其他地方有些许小不同，但都是大同小异，CSDN只是让编写更加人性化更加简单，Github上的编写就比较原始了，必须按照语言规则一点一点的打出来。</p><p><strong>一、Markdown编写工具</strong><br>这里推荐的Markdown编写工具是StackEdit，当然其实我觉得CSDN也可以用来写，但我们还是返璞归真追求真正的Markdown编写比较好。这里的StackEdit不需要下载什么软件可以直接网页直接编写，这里贴出StackEdit工具的网址<a href="https://stackedit.io/app#">StackEdit工具</a><br><img src="https://img-blog.csdnimg.cn/20201007121120800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70#pic_center"></p><p><strong>二、语言规则</strong><br>这边简要概述一些基本常用的语言编写规则<br>1、标题Size：<br>#一级大标题<br>##二级大标题<br>###三级大标题<br>以此类推</p><p>2、字体加粗：<br><img src="https://img-blog.csdnimg.cn/20201007121724748.png#pic_center"><br>在加粗字前后加两个星号**</p><p>3、标点<br>-内容（一个负号符号后面加内容即可在内容前加点号）</p><p>4、添加图片<br>先把要的图片传到Issues中得到url链接，底下有个Attach files上传图片，之后得到如下链接即图片链接，然后复制链接到Readme文件中即可显示图片<br><img src="https://img-blog.csdnimg.cn/20201007121951658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70#pic_center"><br>5、表格形式<br>如下图输入竖号即可形成表格<br><img src="https://img-blog.csdnimg.cn/20201007122107813.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70#pic_center"><br><strong>三、结语</strong><br>以上只是个人总结的简单Readme的编写，具体还可以使Readme内容更加美化，详情可以参考网上资源，本文内容甚少适用于简单的Readme编写，如有不当之处还请谅解。</p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习：深度学习总体概述</title>
      <link href="2021/05/01/shen-du-xue-xi-shen-du-xue-xi-zong-ti-gai-shu/"/>
      <url>2021/05/01/shen-du-xue-xi-shen-du-xue-xi-zong-ti-gai-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在学习深度学习和机器学习也有一段时间了终于有点时间来写写笔记回顾一下之前学到的，本文主要是笔者在阅读和观看一系列深度学习相关的书籍或者视频之后写下自己对深度学习的总体内容概述，如有不全或错误之处还请谅解并指出。</p><h2 id="一、神经网络"><a href="#一、神经网络" class="headerlink" title="一、神经网络"></a><strong>一、神经网络</strong></h2><p>深度学习事实上本质是在研究神经网络，其实深度学习本来就是机器学习的一个分支而机器学习分支中的神经网络由于发展较为迅速并逐渐形成新的学名——深度学习，神经网络和传统的机器学习算法其实个人认为主要的区别是在速度上和准确率上，传统的机器学习算法是在研究各种特征细节，人为能够很好的看到和知道其中的算法原理以及为什么，但深度学习给人的感觉就是一个黑匣子一样，你只知道输入和输出什么，以及中间过程要提供什么样的核等，但对中间计算处理过程很不清楚很模糊。总体而言，个人人为神经网络不仅可以做数据拟合还可以做目标分类识别等，它的能力比以往传统的机器学习还要强大。</p><h2 id="二、前向神经网络"><a href="#二、前向神经网络" class="headerlink" title="二、前向神经网络"></a><strong>二、前向神经网络</strong></h2><p>深度学习中的第一部分前向神经网络其实可以分为多层感知机，受限玻尔兹曼机，卷积神经网络，深度残差网络等<br><strong>1、卷积神经网络</strong><br>卷积神经网络的兴起应该是在ImageNet比赛中，应该是2012年的AlexNet卷积神经网络让世界耳目一新，但其实卷积神经网络很早就已经有了雏形，只不过在早些年的硬件技术上和计算能力上根本满足不了网络的实现。典型的CNN模型有AlexNet，LeNet，VGGNet，ResNet等等，这些都是过去几年来很经典并且效果极好的网络模型。<br>卷积神经网络的发明真的很好的解决了高分辨率图像中参数多，复杂的问题，因为在之前的最基础的神经网络也就是BP神经网络，其输入神经元和中间层都是以全连接的方式进行，这样在以图像为输入数据的神经网络中就会有一个问题，如果图像较大那么平铺展开图像输入进网络那简单的几层网络就会有大量的参数，由此产生了卷积神经网络这种以共享权值作为参数核的网络结构，并且效果好和高效，解决了全连接网络参数多的问题。<br><strong>2、多层感知机</strong><br>多层感知机就是最早的全连接神经网络，只有简单的几层网络，如图所示<br><img src="https://img-blog.csdnimg.cn/2020112821035464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20201128210446521.png"></p><h2 id="三、循环神经网络"><a href="#三、循环神经网络" class="headerlink" title="三、循环神经网络"></a><strong>三、循环神经网络</strong></h2><p>循环神经网络是一类用于处理序列数据结构的网络结构，输入通常是连续长度不固定的序列数据，循环神经网络主要处理序列信息，并且捕获样本之间的关联信息，循环神经网络往往在NLP自然语言处理领域有着较为大的发挥，当然还有语音识别，文字预测等，下图是普通的RNN的一个网络结构图<br><img src="https://img-blog.csdnimg.cn/20201128210852606.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>1、LSTM长短时记忆模型</strong><br>LSTM最主要的是不仅能够记忆短时信息，而且还能对有用的信息进行长期记忆过滤无用信息，提升了网络的学习能力<br><img src="https://img-blog.csdnimg.cn/20201128211621468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>2、GRU门控网络</strong><br>GRU是对LSTM循环神经网络的一个变型，主要是解决了LSTM的梯度消失和爆炸问题<br><img src="https://img-blog.csdnimg.cn/20201128211713925.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"></p><h2 id="四、激活函数"><a href="#四、激活函数" class="headerlink" title="四、激活函数"></a><strong>四、激活函数</strong></h2><p>个人认为激活函数的目的主要是为了使得网络能够具有非线性能力，能够使得网络拟合数据，如果没有激活函数只有单纯的连接，那么网络不管怎么构建都只是一个线性网络，这样的神经网络就失去了它最本质的核心。<br><strong>1、Sigmoid激活函数</strong><br>Sigmoid函数的表达式<br><img src="https://img-blog.csdnimg.cn/20201128213429173.png"></p><p>数学图形式<br><img src="https://img-blog.csdnimg.cn/20201128212254782.png"><br>可以看到其数学特性，输出在0-1之间，输入为任意参数<br>但是Sigmoid函数的缺点就是存在梯度消失或梯度爆炸的问题，而且不仅如此，sigmoid函数的输入在-4，+4之间变换较为大，但超出该范围的输出变换基本将趋于0，这样在数据拟合上就显得更加局限，<br><strong>2、tanh函数</strong><br>tanh函数数学表达式和图形<br><img src="https://img-blog.csdnimg.cn/20201128213417647.png"><br><img src="https://img-blog.csdnimg.cn/20201128213451425.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>可以看出来tanh函数其实就是sigmoid的拓展版，它在输出上具有更大的变化性，但是和sigmoid函数一样还是存在着梯度爆炸和消失的问题<br><strong>3、ReLu函数</strong><br>ReLu函数可以说是深度学习或神经网络中特别常用的一个函数它的构造比较简单<br><img src="https://img-blog.csdnimg.cn/2020112821364085.png"><br><img src="https://img-blog.csdnimg.cn/2020112821370133.png"><br>可以看出在输入小于0的时候其输出为0，它的梯度较为平稳，不管怎么叠加，不会出现梯度消失或者爆炸问题，但是这也存在另一个问题，它的负领域可能会出现神经元死亡的问题，由此产生了变形的Relu函数比如Leaky Relu等</p><h2 id="五、硬件和软件"><a href="#五、硬件和软件" class="headerlink" title="五、硬件和软件"></a>五、硬件和软件</h2><p>深度学习之所以能够迅猛发展，离不开强大的硬件基础，那就是GPU和CPU革命性的提升，众所周知，目前神经网络主要是在GPU上进行计算，CPU上也有但是总体比较少，因为CPU上计算速度会比GPU慢很多，大约可能会差到10-40倍速度，还有个人认为说CPU是在串行计算上占据优势，而GPU在并行计算上占有占有优势，可以去看一下CPU和GPU的硬件参数，可以发现GPU的核心数是可以达到上千个，而CPU远不如GPU只有几个，再者，我认为神经网络很大程度上是在做矩阵的运算因此GPU在这里发挥出了他巨大的优势，目前深度学习主流的GPU就是NVIDIA，我可以说目前深度学习的GPU硬件上被NVIDIA垄断了，但不得不承认它们家确实做的很好，像现在推出的专业人工智能计算版GPU（TESLA P4和V100，TITAN系列等等）价格不菲。<br>在软件平台上其实就是深度学习框架，深度学习很大一部分已被Python语言占据了，现在主流的框架都是以Python为主，很多深度学习框架一开始源于高校后来逐渐走向商业化，比如Theano，caffe，Torch，Keras，Tensorflow，PaddlePaddle等等。</p><h2 id="六、其余的神经网络"><a href="#六、其余的神经网络" class="headerlink" title="六、其余的神经网络"></a>六、其余的神经网络</h2><p>由于这些精力有限所以这些神经网络还是不太熟悉所以暂且先罗列出来<br><strong>1、生成式对抗神经网络<br>2、图神经网络<br>3、强化学习：Q-learning</strong></p><h2 id="七、参考文献"><a href="#七、参考文献" class="headerlink" title="七、参考文献"></a>七、参考文献</h2><p><strong>1、CS231n斯坦福大学公开课</strong><br><strong>2、《百面深度学习》——诸葛越</strong><br><strong>3、《百面机器学习》——诸葛越</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读：Unprocessing Images for Learned Raw Denoising</title>
      <link href="2021/05/01/lun-wen-yue-du-unprocessing-images-for-learned-raw-denoising/"/>
      <url>2021/05/01/lun-wen-yue-du-unprocessing-images-for-learned-raw-denoising/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文主要是对《Unprocessing Images for Learned Raw Denoising》论文进行部分翻译解读，如果有观点不对或者错误的地方还望谅解并指出。</p><h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>首先谈谈本文的一个基线，本文的目的是通过sRGB模拟转换成RAW图输入神经网络中进行降噪然后再转换回sRGB图像形成最终降噪后的图像。本文的最终目的是降噪但是亮点（或核心）不在降噪而是在于sRGB图模拟转换成RAW图，通过阅读该论文可以看出论文中有相当一大部分是在说“unprocess”也就是sRGB形成RAW图的过程，差不多占到快一半的部分吧。以下是整个论文的算法架构。<br><img src="https://img-blog.csdnimg.cn/20201201135230967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"></p><h2 id="二、Unprocess"><a href="#二、Unprocess" class="headerlink" title="二、Unprocess"></a>二、Unprocess</h2><p>输入的sRGB图像总共是三通道，经过Unprocess后输出的图像总共是四通道的图像，具体这四通道的图像是什么样呢？其实就是Bayer拜耳图像，可以通过百度进一步了解Bayer图像，这里只是简单介绍Bayer图像。输出的四通道图像分别是R,Gr，Gb，B这四个通道矩阵，Bayer格式的图像其实就是相机内部最原始的图像，这也是为什么要输出四通道的原因，如下图所示可以看出Bayer图像中RGB阵列是有规律的进行排布。可以看出偶数行输出是GBGB而奇数行输出是RGRG（第一列或第一行是0行/列所以作为偶数行/列），这也是论文中转换成Bayer RGB图像的基础，取偶数行和奇数行的GBGB和RGRG。<br><img src="https://img-blog.csdnimg.cn/20201201141556309.png"><br><strong>1、Invert Tone Mapping色调映射的反变换</strong><br>所谓色调映射就是调整图像的整体灰度，使得图像更加均衡并且使原有信息更加清楚的表达出来这里借一张图来说明解释<br><img src="https://img-blog.csdnimg.cn/20201201142830835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>而对色调映射进行反变换后，显而易见图像的整体亮度和清晰度也会随之减小，该论文中使用的色调映射算法公式如下<br><img src="https://img-blog.csdnimg.cn/20201201142942741.png"><br><strong>2、Gamma Decompression伽马解压</strong><br>这里其实就是伽马矫正公式的反变换，伽马矫正也是调整图像整体灰度分布和亮度的一个操作，伽马矫正的公式和伽马矫正反变换的公式如下<br><img src="https://img-blog.csdnimg.cn/20201201143322888.png"><img src="https://img-blog.csdnimg.cn/20201201143341949.png"><br><strong>3、Color Correction色彩校正</strong><br>相机应用一个3x3颜色校正矩阵(CCM)来将自己的相机空间RGB颜色测量转换为sRGB值。数据集由四个摄像头组成，每个摄像头在进行色彩校正时使用自己的固定CCM。为了生成能够推广到数据集中所有相机的合成数据，我们对这四种CCM的随机凸组合进行采样，对于每一幅合成图像，我们应用采样CCM的逆来撤销颜色校正的效果。<br><strong>4、White Balance白平衡</strong><br>白平衡主要的作用是，由于在不同的色温，光照下图像会出现较大的偏色因此需要白平衡算法进行处理消除影响。白平衡的算法有很多种，本文中使用的白平衡算法主要是通过RGB三通道的值乘上各自的增益即可。本文中Red的增益在[1.9,2.4]，Blue的增益在[1.5,1.9]，当G增益&gt;1，x&gt;t时<br><img src="https://img-blog.csdnimg.cn/2020120115064133.png"><br><strong>5、Demosaicing去马赛克</strong><br>这里其实就是RAW图中Bayer RGB格式和sRGB格式的转换，传统相机传感器中的每个像素都由一个红色、绿色或蓝色滤光片覆盖，滤光片按拜耳模式排列R-Gr-Gb-B，转换方法在提头中有提到，sRGB向Bayer的转换只要提取相应偶数和奇数行/列即可</p><h2 id="三、UNet神经网络结构"><a href="#三、UNet神经网络结构" class="headerlink" title="三、UNet神经网络结构"></a>三、UNet神经网络结构</h2><p><img src="https://img-blog.csdnimg.cn/20201201151015985.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>如图所示本文中的网络结构清晰可见，其中下采样使用池化实现，上采样使用插值实现，最后网络输出的图像和原始图像进行相加得到最终降噪RAW图，其中损失函数的计算是在process成sRGB图像后进行计算而非在RAW图上进行计算</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>在原论文中，作者谈到为什么要进行RAW图像转换，其根本目的是为了验证由于现在大多图像都是由人工合成不真实的图像训练数据会影响到算法的准确性，因此进行模拟相机RAW图再进行数据训练会提高结果的准确性和可信度。</p><h2 id="五、参考文献"><a href="#五、参考文献" class="headerlink" title="五、参考文献"></a>五、参考文献</h2><p>[1]<a href="https://blog.csdn.net/u013049912/article/details/84997826">Unprocessing Images for Learned Raw Denoising原文翻译</a><br>[2]<a href="https://chenjunkai.blog.csdn.net/article/details/106882131">Unprocessing Images for Learned Raw Denoising论文解读</a><br>[3]<a href="https://blog.csdn.net/qingzhuyuxian/article/details/82965054">Bayer图像格式介绍</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ISP </tag>
            
            <tag> 去噪 </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习：Pytorch训练数据时问题总结</title>
      <link href="2021/05/01/shen-du-xue-xi-pytorch-xun-lian-shu-ju-shi-wen-ti-zong-jie/"/>
      <url>2021/05/01/shen-du-xue-xi-pytorch-xun-lian-shu-ju-shi-wen-ti-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文是在笔者使用Pytorch深度学习框架训练自己的网络时遇到的一些问题，在此一并记录下来作为学习笔记，本文中的问题主要是Pytorch框架上遇到的一些问题而非单独针对个人网络代码框架上的问题</p><h2 id="问题一、"><a href="#问题一、" class="headerlink" title="问题一、"></a>问题一、</h2><p><strong>问题</strong>：在使用Pytorch加载训练好的模型文件checkpoint.tar或其它保存类型的模型文件时，如果遇到“xxxxx is a zip archive (did you mean to torch.jit.load()?)”，大概就是这样一回事<br><strong>分析</strong>：这里大概意思就是说你这个版本的Pytorch无法读入这个你这个在高版本运行出来保存的模型文件，在高版本的Pytorch的torch.save方法中已经把旧的保存的模型文件方法换成了一种新的基于zip的保存方法，而这时候你的旧版本Pytorch无法读入高版本产生的模型文件；特此说明，自从torch1.6及之后开始，Pytorch就更换了保存模型文件的方法，见下面官方图<br><img src="https://img-blog.csdnimg.cn/20201218210342460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><strong>解决方法</strong>：如果你坚持要用旧版的保存模型方法，OK可以，但是你得在torch.save后面加上_use_new_zipfile_serialization=False，意思就是不使用新的zip保存方法，比如</p><pre class="line-numbers language-none"><code class="language-none">torch.save(model, _use_new_zipfile_serialization&#x3D;False)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h2><p><strong>问题</strong>：在训练模型的时候遇到RuntimeError:CUDA out of Memory等等，然后说allocate xxxx MB/GB （but xxxxxxx），总的来说就是你要训练的模型所需要的GPU显存空间不足<br><strong>分析</strong>：一般整个训练代码中，耗GPU内存最多的就是网络结构，而网络结构中最耗内存的就是参数和激活函数产生的中间变量等，当然还有可能是你的单个数据图像的过大或者你一次处理的batch_size太大<br><strong>解决方法</strong>：1、减小Batch_Size和数据大小  2、查看在测试数据部分的代码中是否加上with torch.no_grad表示在测试数据时不进行反向传播和梯度计算 3、在训练数据代码部分前加上释放GPU数据的代码</p><h2 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h2><p><strong>问题</strong>：在命令行窗口下直接运行所要执行的代码即直接<code>python train.py</code>发生报错：ModuleNotFoundError，当让这种情况是在你train.py代码中还import了你自己写的其他py文件代码<br><strong>分析</strong>：在运行Python的IDE中如果直接运行代码文件，OK没问题那是因为在软件IDE条件下，软件会帮你把你一个文件夹下其他的代码文件的sys.path设置好加入进去，但是如果在命令行下面运行就不一样没人帮你做这件事情，在IDE中import都是相对路径而不是绝对路径因而我们要做的就是在执行代码的头部加上一段代码使得其他文件能够加入sys的path中，根据import的机制，代码就会相对应找到相应的包（import机制详情请自行搜索查询）<br><strong>解决方法</strong>：请在你要执行的代码最前面加上如下代码</p><pre class="line-numbers language-none"><code class="language-none">import sysimport osif __name__ &#x3D;&#x3D; &#39;__main__&#39;:       sys.path.append(os.path.dirname(sys.path[0]))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="问题四"><a href="#问题四" class="headerlink" title="问题四"></a>问题四</h2><p><strong>问题</strong>：运行测试数据代码的时候报错，RuntimeError: Attempting to deserialize object on CUDA device 2 but torch.cuda.device_count() is 1<br><strong>分析</strong>：大概就是说你训练的时候使用两个GPU训练但是现在你电脑只有一个GPU<br><strong>解决方法</strong>：在torch.load的时候后面加上map_location=’cuda:0’，比如</p><pre class="line-numbers language-none"><code class="language-none">torch.load(args.load_model, map_location&#x3D;&#39;cuda:0&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="问题五"><a href="#问题五" class="headerlink" title="问题五"></a>问题五</h2><p><strong>问题</strong>：具体的内容我给忘记了没有记下来，但是是在测试数据集代码运行时报错，大概内容是测试数据的张量大小是三维不符合模型中的四维数据模型<br><strong>分析</strong>：在训练数据的时候数据都是以四维的形式即[batch, channel, H, W]输入进去，但是在测试数据的时候虽然输入的图像数据是三维的大小但是你需要同样的把它变换成四维数据<br><strong>解决方法</strong>：给输入数据加上unsqueeze即<code>input_data = input_data.unsqueeze(0)</code>，0就是在第一维上添加一个1，输入就会变成[1, *, *, *]</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习：GPU云服务器的租用</title>
      <link href="2021/05/01/shen-du-xue-xi-gpu-yun-fu-wu-qi-de-zu-yong/"/>
      <url>2021/05/01/shen-du-xue-xi-gpu-yun-fu-wu-qi-de-zu-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文是在笔者做深度学习相关研究的时候需要高算力GPU去运行代码因而选择去租借GPU云服务器，这里记下自己所接触的一些GPU服务器网站和使用技巧</p><h2 id="一、推荐站点"><a href="#一、推荐站点" class="headerlink" title="一、推荐站点"></a>一、推荐站点</h2><p>1、<a href="https://cloud.videojj.com/">极链AI云</a>，首当其冲的一定是这家，这家是我用过认为较为好用而且相对价格比较平均然后操作方便，还有最重要的一点就是它微信注册绑定送100元，如果是学生再送100元，真的很爽，而且这家的GPU种类比较多，从RTX2080Ti，Tesla P100，Tesla T4，Ampere A100等，还是比较多的种类的<br>2、<a href="https://www.1024gpu.top/home">1024Lab云</a>，这家的话服务器是架设在国外的，租用界面使用Github上一个开源的GPU租用界面，这家的话是所有租用平台上最便宜的但是它的种类比较少，最好的就只有2080Ti，最差的就是1080，但是2080Ti显卡的价格是0.215美刀/小时，真的是最便宜的了，其他国内平台像RTX2080Ti都是需要￥3/h左右的，但是这家有个缺点就是它的交易是通过网上货币DBC进行支付而且这家的话注册送的DBC不多只有1000DBC算比较少，只能用个几小时<br>3、<a href="https://www.matpool.com/host-market">矩池云GPU</a>，这家的话做的会比较大，和很多的高校和公司都有过合作所以它的价格必然会高一些，但是这家在开始的时候会送5块钱的券，而且这家的话优点在于它可以VNC远程访问图形化桌面，操作简单，而且他家的GPU种类是真的很多啊<br>4、<a href="https://www.easyaiforum.cn/">易学智能AI云</a>这家的特点是贵，个人感觉很贵，然后它的优点就是真的真的很好操作而且很人性化等等，具体我就不介绍了<br>5、<a href="https://mistgpu.com/">MistGPU云</a>，这家的特点就是便宜了，注册送8块钱券，便宜的话就是差不多一个tesla P100只要4块钱/小时，邀请他人使用注册的会再送8块钱，无上限</p><h2 id="二、GPU云服务器使用"><a href="#二、GPU云服务器使用" class="headerlink" title="二、GPU云服务器使用"></a>二、GPU云服务器使用</h2><p><strong>1、访问工具</strong><br>访问GPU云服务器的方式无非两种，一种是命令行shell访问另一种就是图形化界面访问，但是大多好像都是只提供shell命令行访问即终端访问，可能是在云服务器端要每个用户都配备图形化界面消耗的空间和功率更大吧<br>终端访问工具：Xshell（这是最常用的终端访问），Jupyter（也挺好用的），Xftp（主机和云服务器间进行FTP传输文件的工具）<br><img src="https://img-blog.csdnimg.cn/20201218222527832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70" alt="Xshel界面"><br><img src="https://img-blog.csdnimg.cn/20201218222626978.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>图形化界面访问工具：Teamviewer（个人觉得挺好用的），向日葵，VNC，这些第三方软件都需要主机和云服务器都要安装相应软件才可以互相进行访问控制和文件传输<br><strong>2、使用方法</strong><br>这里将以极链云AI平台使用介绍，其实都大同小异的，主要访问工具是Xshell<br>（1）在极链云上选择服务器进行租用<br><img src="https://img-blog.csdnimg.cn/20201218223208129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>（2）选择完成后系统产生GPU实例<br><img src="https://img-blog.csdnimg.cn/202012182234070.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20201218223434761.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>（3）打开Xshell软件进行配置连接<br><img src="https://img-blog.csdnimg.cn/20201218223550980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>在 新建会话属性 窗口填写自定义名称，主机 填写从控制台复制出的登录指令域名部分，端口号 填写 -p 后的数字<br><img src="https://img-blog.csdnimg.cn/20201218223610391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>点击左侧类别中的 用户身份认证，右侧 用户名 填写 root，密码填写从控制台复制出的密码。确认保存<br><img src="https://img-blog.csdnimg.cn/2020121822364271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>打开并连接<br><img src="https://img-blog.csdnimg.cn/20201218223654658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20201218223718823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20201218223729310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>类似的Xftp也是相同的操作<br><strong>3、文件传输方法</strong><br>（1）使用各家平台的网盘进行传输，比如极链有提供自己的网盘，可以在启动服务器前先上传数据到网盘上，然后在机器对应的目录下去访问读取数据<br><img src="https://img-blog.csdnimg.cn/2020121822421112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzExNjk3,size_16,color_FFFFFF,t_70"><br>（2）使用FTP传输文件，可以使用Xftp传输但是普遍速度很慢，也可以使用FileZilla，具体使用方法自行百度</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]<a href="https://cloud.videojj.com/help/docs/data_manage.html#id4">极链AI云帮助文档</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPU </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 云服务器 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
